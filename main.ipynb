{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "f7133f1d",
         "metadata": {},
         "source": [
            "## Training target model\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "478c05d3",
         "metadata": {},
         "source": [
            "### Introduction"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "abce8ff6",
         "metadata": {},
         "source": [
            "1. Pull the latest version of repository - to see, which models were already chosen by others.\n",
            "2. Install new `requirements.txt`. It should work with python 3.12 and upgraded pip. It contains cuda tensrflow version. If you have problems with installation, try with `requirements-no-gpu.txt`.\n",
            "3. Check, if tensorflow sees your graphics card.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "ba232e4c",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2026-02-05 18:32:30.523310: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                  "2026-02-05 18:32:30.571938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                  "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                  "2026-02-05 18:32:32.433130: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                  "/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
                  "  if not hasattr(np, \"object\"):\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
                  ]
               },
               "execution_count": 1,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Source - https://stackoverflow.com/a\n",
            "# Posted by Wilmar van Ommeren, modified by community. See post 'Timeline' for change history\n",
            "# Retrieved 2026-01-18, License - CC BY-SA 4.0\n",
            "\n",
            "import tensorflow as tf\n",
            "tf.config.list_physical_devices('GPU')"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bf9dafc6",
         "metadata": {},
         "source": [
            "it should return something like:\n",
            "\n",
            "```\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "```\n",
            "\n",
            "4. Running model involves preparing GPU, downloading data and running appropriate training. Everything is described below.\n",
            "5. And don't forget to push your name next to chosen model to repository!\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9c4ce42f",
         "metadata": {},
         "source": [
            "#### Use the whole VRAM\n",
            "\n",
            "Set VRAM size accoring to your GPU!!\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "c176fb3f",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import tf\n",
            "\n",
            "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
            "if gpus:\n",
            "    try:\n",
            "        for gpu in gpus:\n",
            "            tf.config.experimental.set_memory_growth(gpu, False)\n",
            "        tf.config.set_logical_device_configuration(\n",
            "            gpus[0],\n",
            "            [tf.config.LogicalDeviceConfiguration(memory_limit=4000)] # Limit to 4000 MB of VRAM - TODO: adjust based on your GPU\n",
            "        )\n",
            "    except RuntimeError as e:\n",
            "        print(e)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9ac5dc5f",
         "metadata": {},
         "source": [
            "### Loading data and training model\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "186010ba",
         "metadata": {},
         "source": [
            "Model training subsections are divided by grid density. Choose one model training subsection and then please clear cell outputs, **ADD your name in chosen model section header** and push this change on the repository. This should help us avoid conflicts, that 2 people will choose the same model.\n",
            "\n",
            "So the header sholud like this:\n",
            "\n",
            "```markdown\n",
            "##### **GRZEGORZ** 1. Model - shallowed unet 256x256, grid density 1 m\n",
            "```\n",
            "\n",
            "#### trainer.random_fit_from_files() arguments\n",
            "\n",
            "At the moment `epoch_steps` and `epochs` are set to 1. Run it and check, how long does it take to execute it. Then adjust these values to the amout of time you have. `epoch_steps` over 200 probably doesn't make sense, so later on increase just `epochs`.\n",
            "\n",
            "If it lasts over 15 minutes, it means that probably there is some problem with graphics card. Nevertheless, it should run properly. Model is dumped to a different the file after each training and starts from the last checkpoint everytime, so don't hesitate to run it multiple times, if you want/have time to do it.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "2f87fb9a",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "\n",
            "data_dir = os.path.join(\"grids\", \"with-is-residential\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c2779acc",
         "metadata": {},
         "source": [
            "#### Grid density 1m\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "56067167",
         "metadata": {},
         "source": [
            "##### Downloading cities\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "051c35b1",
         "metadata": {},
         "source": [
            "If you have limited disk space, you can adjust `random_cities_count` variable, to download less city grids. It will save time and disk space, but will probably affect model training.\n",
            "If you do this, please write somewhere down, which cities where used to train model - it will help when writing report.\n",
            "\n",
            "> **Note:** It lasts for a while - for Częstochowa it is up to 20 minutes\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "09380f82",
         "metadata": {},
         "outputs": [],
         "source": [
            "import random\n",
            "from scraper.data_loader import DataLoader\n",
            "from trainer.model import Model\n",
            "from trainer.trainer import Trainer\n",
            "import os\n",
            "from pathlib import Path\n",
            "\n",
            "random_cities_count = 11\n",
            "\n",
            "folder_path = Path(data_dir)\n",
            "folder_path.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "\n",
            "# Prefered cities to download\n",
            "cities = [\n",
            "    (\"Gdańsk, Polska\", 466630),\n",
            "    (\"Bydgoszcz, Polska\", 358928),\n",
            "    (\"Lublin, Polska\", 339433),\n",
            "    (\"Gdynia, Polska\", 246635),\n",
            "    (\"Radom, Polska\", 214000),\n",
            "    (\"Toruń, Polska\", 202074),\n",
            "    (\"Świętochłowice, Polska\", 55_000),\n",
            "    (\"Koszalin, Polska\", 108_000),\n",
            "    (\"Mielec, Polska\", 59_000),\n",
            "    (\"Chorzów, Polska\", 110_000),\n",
            "    (\"Rybnik, Polska\", 140_000)\n",
            "]\n",
            "\n",
            "# List of Polish cities with population between 50,000 and 500,000\n",
            "# cities = [\n",
            "#     # (\"Warsaw, Polska\", 1790658),\n",
            "#     # (\"Kraków, Polska\", 780981),\n",
            "#     # (\"Łódź, Polska\", 687702),\n",
            "#     # (\"Wrocław, Polska\", 640648),\n",
            "#     # (\"Poznań, Polska\", 538633),\n",
            "#     # (\"Gdańsk, Polska\", 466630),\n",
            "#     (\"Szczecin, Polska\", 402465),\n",
            "#     # (\"Bydgoszcz, Polska\", 358928),\n",
            "#     (\"Lublin, Polska\", 339433),\n",
            "#     (\"Białystok, Polska\", 297459),\n",
            "#     (\"Katowice, Polska\", 294510),\n",
            "#     # (\"Gdynia, Polska\", 246635),\n",
            "#     (\"Częstochowa, Polska\", 224000),\n",
            "#     (\"Radom, Polska\", 214000),\n",
            "#     (\"Toruń, Polska\", 202074),\n",
            "#     (\"Kielce, Polska\", 200000),\n",
            "#     (\"Rzeszów, Polska\", 196000),\n",
            "#     (\"Opole, Polska\", 128000),\n",
            "#     (\"Gliwice, Polska\", 180000),\n",
            "#     (\"Zabrze, Polska\", 170000),\n",
            "#     (\"Elbląg, Polska\", 120000),\n",
            "#     (\"Płock, Polska\", 120000),\n",
            "#     (\"Nowy Sącz, Polska\", 85_000),\n",
            "#     (\"Słupsk, Polska\", 91_000),\n",
            "#     # (\"Świętochłowice, Polska\", 55_000),\n",
            "#     (\"Jelenia Góra, Polska\", 79_000),\n",
            "#     (\"Stalowa Wola, Polska\", 75_000),\n",
            "#     # (\"Koszalin, Polska\", 108_000),\n",
            "#     (\"Mielec, Polska\", 59_000),\n",
            "#     (\"Legnica, Polska\", 100_000),\n",
            "#     (\"Tychy, Polska\", 130_000),\n",
            "#     (\"Chorzów, Polska\", 110_000),\n",
            "#     # (\"Rybnik, Polska\", 140_000)\n",
            "# ]\n",
            "\n",
            "# Randomly select 8 cities\n",
            "random_cities = random.sample(cities, random_cities_count)\n",
            "\n",
            "loader = DataLoader(1, 5000, 5000, data_dir=data_dir, input_third_dimension=3)\n",
            "managers = []\n",
            "files = []\n",
            "\n",
            "for city, population in random_cities:\n",
            "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
            "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
            "    manager = loader.load_city_grid(city,file_name)\n",
            "    files.append(file_name)\n",
            "    managers.append(manager)\n",
            "    loader.add_elevation_to_grid(manager)\n",
            "    loader.add_residential_to_grid(manager)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "fe9c3063",
         "metadata": {},
         "source": [
            "Version used by Jakub in \"only is_street\" dimension in input data model training tests - below:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "6b7cfd37",
         "metadata": {},
         "outputs": [],
         "source": [
            "import random\n",
            "from scraper.data_loader import DataLoader\n",
            "from trainer.model import Model\n",
            "from trainer.trainer import Trainer\n",
            "import os\n",
            "from pathlib import Path\n",
            "\n",
            "random_cities_count = 11\n",
            "\n",
            "folder_path = Path(data_dir)\n",
            "folder_path.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "\n",
            "# Prefered cities to download\n",
            "cities = [\n",
            "    (\"Gdańsk, Polska\", 466630),\n",
            "    (\"Bydgoszcz, Polska\", 358928),\n",
            "    (\"Lublin, Polska\", 339433),\n",
            "    (\"Gdynia, Polska\", 246635),\n",
            "    (\"Radom, Polska\", 214000),\n",
            "    (\"Toruń, Polska\", 202074),\n",
            "    (\"Świętochłowice, Polska\", 55_000),\n",
            "    (\"Koszalin, Polska\", 108_000),\n",
            "    (\"Mielec, Polska\", 59_000),\n",
            "    (\"Chorzów, Polska\", 110_000),\n",
            "    (\"Rybnik, Polska\", 140_000)\n",
            "]\n",
            "\n",
            "\n",
            "random_cities = random.sample(cities, random_cities_count)\n",
            "\n",
            "loader = DataLoader(1, 5000, 5000, data_dir=data_dir, input_third_dimension=3) # not =1 because it does not work then\n",
            "managers = []\n",
            "files = []\n",
            "\n",
            "for city, population in random_cities:\n",
            "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
            "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
            "    manager = loader.load_city_grid(city,file_name)\n",
            "    files.append(file_name)\n",
            "    managers.append(manager)\n",
            "    #loader.add_elevation_to_grid(manager)\n",
            "    #loader.add_residential_to_grid(manager)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2145c4a8",
         "metadata": {},
         "outputs": [],
         "source": [
            "import math\n",
            "import numpy as np\n",
            "from grid_manager import GridManager\n",
            "\n",
            "\n",
            "def add_zero_dimension_third_channel(grid_manager: GridManager, dimension_index: int):\n",
            "    meta = grid_manager.get_metadata()\n",
            "\n",
            "    segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
            "    segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
            "\n",
            "    for row_idx in range(segments_rows):\n",
            "        for col_idx in range(segments_cols):\n",
            "            segment = grid_manager.read_segment(row_idx, col_idx)\n",
            "\n",
            "            h, w, _ = segment.shape\n",
            "\n",
            "            for y in range(h):\n",
            "                for x in range(w):\n",
            "                    segment[y, x, dimension_index] = 0\n",
            "\n",
            "            grid_manager.write_segment(segment, row_idx, col_idx)\n",
            "\n",
            "            print(f\"Segment [{row_idx}, {col_idx}] saved. Max value in dimension {dimension_index}: {np.max(segment[:, :, dimension_index]):.2f}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8e0fb4e0",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "from pathlib import Path\n",
            "import random\n",
            "\n",
            "random_cities_count = 11\n",
            "\n",
            "folder = Path(data_dir)\n",
            "folder.mkdir(parents=True, exist_ok=True)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "cities = [\n",
            "    (\"Gdańsk, Polska\", 466630),\n",
            "    (\"Bydgoszcz, Polska\", 358928),\n",
            "    (\"Lublin, Polska\", 339433),\n",
            "    (\"Gdynia, Polska\", 246635),\n",
            "    (\"Radom, Polska\", 214000),\n",
            "    (\"Toruń, Polska\", 202074),\n",
            "    (\"Świętochłowice, Polska\", 55_000),\n",
            "    (\"Koszalin, Polska\", 108_000),\n",
            "    (\"Mielec, Polska\", 59_000),\n",
            "    (\"Chorzów, Polska\", 110_000),\n",
            "    (\"Rybnik, Polska\", 140_000)\n",
            "]\n",
            "\n",
            "random_cities = random.sample(cities, random_cities_count)\n",
            "for city, population in random_cities:\n",
            "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
            "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
            "    grid_manager = GridManager(file_name, data_dir=data_dir)\n",
            "    add_zero_dimension_third_channel(grid_manager, dimension_index=1)\n",
            "    add_zero_dimension_third_channel(grid_manager, dimension_index=2)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "942e32d6",
         "metadata": {},
         "source": [
            "##### **GRZEGORZ** 1.1. Model - shallowed unet 256x256, grid density 1 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4794dadf",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "from grid_manager import GridManager\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "64d34e2c",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=50,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "d252fecb",
         "metadata": {},
         "source": [
            "##### **JAKUB** 1.2. Model - shallowed unet 256x256, grid density 1 m - **no altitude**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "00fbc81a",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                  "I0000 00:00:1770312878.334878   19054 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
                  "I0000 00:00:1770312878.349473   19054 cuda_executor.cc:508] failed to allocate 3.91GiB (4194304000 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "files: [<grid_manager.GridManager object at 0x7fad9fca91f0>, <grid_manager.GridManager object at 0x7fae31380aa0>, <grid_manager.GridManager object at 0x7fad9fa818e0>, <grid_manager.GridManager object at 0x7fad9fa83f50>, <grid_manager.GridManager object at 0x7fad9fa83e90>, <grid_manager.GridManager object at 0x7fad9fa83ef0>, <grid_manager.GridManager object at 0x7fad9fa83da0>, <grid_manager.GridManager object at 0x7fad9fa83fe0>, <grid_manager.GridManager object at 0x7fad9fa83fb0>, <grid_manager.GridManager object at 0x7fad9fa1ec90>, <grid_manager.GridManager object at 0x7fad9facc500>]\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "from grid_manager import GridManager\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    model_type=ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"shallowed_unet_256_1m_is_street_only\"),     # where to save the model\n",
            "\n",
            "    input_third_dimension=1,\n",
            "    output_third_dimension=1\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "317cc6d3",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2026-02-05 18:34:49.557000: I external/local_xla/xla/service/service.cc:163] XLA service 0x7fac80008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
                  "2026-02-05 18:34:49.557013: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
                  "2026-02-05 18:34:49.602458: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
                  "2026-02-05 18:34:50.212495: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91800\n",
                  "2026-02-05 18:35:01.484972: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=4} for conv (f16[64,3,3,192]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,128,128,192]{3,2,1,0}, f16[8,128,128,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
                  "2026-02-05 18:35:03.283224: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.798348599s\n",
                  "Trying algorithm eng19{k2=4} for conv (f16[64,3,3,192]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,128,128,192]{3,2,1,0}, f16[8,128,128,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u001b[1m 1/50\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:29\u001b[0m 23s/step - accuracy: 0.0000e+00 - loss: 7.0296"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "I0000 00:00:1770312908.747430   20177 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u001b[1m49/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.9086 - loss: 2.4979"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2026-02-05 18:36:52.301927: W tensorflow/core/framework/op_kernel.cc:1842] INVALID_ARGUMENT: ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n",
                  "Traceback (most recent call last):\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
                  "    ret = func(*args)\n",
                  "          ^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
                  "    return func(*args, **kwargs)\n",
                  "           ^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
                  "    values = next(generator_state.get_iterator(iterator_id))\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n",
                  "    _, cut = generate_cut(grid, cut_size)\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n",
                  "    result[:, :, [\n",
                  "\n",
                  "ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n",
                  "\n",
                  "\n",
                  "2026-02-05 18:36:52.302056: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n",
                  "Traceback (most recent call last):\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
                  "    ret = func(*args)\n",
                  "          ^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
                  "    return func(*args, **kwargs)\n",
                  "           ^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
                  "    values = next(generator_state.get_iterator(iterator_id))\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n",
                  "    _, cut = generate_cut(grid, cut_size)\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n",
                  "    result[:, :, [\n",
                  "\n",
                  "ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n",
                  "\n",
                  "\n",
                  "\t [[{{node PyFunc}}]]\n",
                  "\t [[IteratorGetNext]]\n",
                  "2026-02-05 18:36:52.302074: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n",
                  "Traceback (most recent call last):\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
                  "    ret = func(*args)\n",
                  "          ^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
                  "    return func(*args, **kwargs)\n",
                  "           ^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
                  "    values = next(generator_state.get_iterator(iterator_id))\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n",
                  "    _, cut = generate_cut(grid, cut_size)\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "\n",
                  "  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n",
                  "    result[:, :, [\n",
                  "\n",
                  "ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n",
                  "\n",
                  "\n",
                  "\t [[{{node PyFunc}}]]\n",
                  "\t [[IteratorGetNext]]\n",
                  "\t [[IteratorGetNext/_2]]\n",
                  "2026-02-05 18:36:52.302089: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 3000858055412023731\n",
                  "2026-02-05 18:36:52.302109: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6747149561929421427\n"
               ]
            },
            {
               "ename": "InvalidArgumentError",
               "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\nTraceback (most recent call last):\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n    _, cut = generate_cut(grid, cut_size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n    result[:, :, [\n\nValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\nTraceback (most recent call last):\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n    _, cut = generate_cut(grid, cut_size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n    result[:, :, [\n\nValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_5345]",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom_fit_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Train the model with random data from files\u001b[39;49;00m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# number of epochs to train\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# number of steps per epoch\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# size of each training batch\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have\u001b[39;00m\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/trainer.py:19\u001b[39m, in \u001b[36mTrainer.random_fit_from_files\u001b[39m\u001b[34m(self, epochs, steps_per_epoch, batch_size)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrandom_fit_from_files\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs: \u001b[38;5;28mint\u001b[39m = \u001b[32m100\u001b[39m, steps_per_epoch=\u001b[32m1000\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m32\u001b[39m):\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform training on model.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m        fits_count (int): Number of fits to perform.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_files\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcut_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_clipping_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_clipping_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipping_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_clipping_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_surplus\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_grid_surplus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m._model.save()\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/clipping_model.py:115\u001b[39m, in \u001b[36mClippingModel.fit\u001b[39m\u001b[34m(self, train_files, val_files, cut_sizes, clipping_size, input_surplus, batch_size, epochs, steps_per_epoch)\u001b[39m\n\u001b[32m    113\u001b[39m val_dataset = get_tf_dataset(val_files, cut_sizes, clipping_size, input_surplus, batch_size, input_third_dimension=\u001b[38;5;28mself\u001b[39m.input_third_dimension, output_third_dimension=\u001b[38;5;28mself\u001b[39m.output_third_dimension)\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Fit the model using the datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_keras_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/croada/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                  "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\nTraceback (most recent call last):\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n    _, cut = generate_cut(grid, cut_size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n    result[:, :, [\n\nValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  ValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\nTraceback (most recent call last):\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 62, in clipping_sample_generator\n    _, cut = generate_cut(grid, cut_size)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/jakseluz/nauka/V_semestr/studio_projektowe_1/CRoadA/trainer/data_generator.py\", line 138, in generate_cut\n    result[:, :, [\n\nValueError: shape mismatch: value array of shape (0,256,3) could not be broadcast to indexing result of shape (256,256,3)\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_5345]"
               ]
            }
         ],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=50,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f1d77f05",
         "metadata": {},
         "source": [
            "##### 2. Model unet 256x256, grid density 1 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0ca87032",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_256_1m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "bec9d7b1",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f2c52511",
         "metadata": {},
         "source": [
            "##### **Jakub** 3. Model unet 512x512, grid density 1 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0430e553",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=512,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=128,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_512_1m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "92816269",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "dd455342",
         "metadata": {},
         "source": [
            "#### Grid density 2 m\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0a8e0c91",
         "metadata": {},
         "source": [
            "If you have limited disk space, you can adjust `random_cities_count` variable, to download less city grids. It will save time and disk space, but will probably affect model training.\n",
            "If you do this, please write somewhere down, which cities where used to train model - it will help when writing report.\n",
            "\n",
            "> **Note:** It lasts for a while - for Częstochowa it is up to 20 minutes\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "97215bc7",
         "metadata": {},
         "outputs": [],
         "source": [
            "import random\n",
            "from scraper.data_loader import DataLoader\n",
            "from trainer.model import Model\n",
            "from trainer.trainer import Trainer\n",
            "import os\n",
            "from pathlib import Path\n",
            "\n",
            "random_cities_count = 11\n",
            "\n",
            "folder_path = Path(data_dir)\n",
            "folder_path.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "\n",
            "# Prefered cities to download\n",
            "cities = [\n",
            "    (\"Gdańsk, Polska\", 466630),\n",
            "    (\"Bydgoszcz, Polska\", 358928),\n",
            "    (\"Lublin, Polska\", 339433),\n",
            "    (\"Gdynia, Polska\", 246635),\n",
            "    (\"Radom, Polska\", 214000),\n",
            "    (\"Toruń, Polska\", 202074),\n",
            "    (\"Świętochłowice, Polska\", 55_000),\n",
            "    (\"Koszalin, Polska\", 108_000),\n",
            "    (\"Mielec, Polska\", 59_000),\n",
            "    (\"Chorzów, Polska\", 110_000),\n",
            "    (\"Rybnik, Polska\", 140_000)\n",
            "]\n",
            "\n",
            "# List of Polish cities with population between 50,000 and 500,000\n",
            "# cities = [\n",
            "#     # (\"Warsaw, Polska\", 1790658),\n",
            "#     # (\"Kraków, Polska\", 780981),\n",
            "#     # (\"Łódź, Polska\", 687702),\n",
            "#     # (\"Wrocław, Polska\", 640648),\n",
            "#     # (\"Poznań, Polska\", 538633),\n",
            "#     # (\"Gdańsk, Polska\", 466630),\n",
            "#     (\"Szczecin, Polska\", 402465),\n",
            "#     # (\"Bydgoszcz, Polska\", 358928),\n",
            "#     (\"Lublin, Polska\", 339433),\n",
            "#     (\"Białystok, Polska\", 297459),\n",
            "#     (\"Katowice, Polska\", 294510),\n",
            "#     # (\"Gdynia, Polska\", 246635),\n",
            "#     (\"Częstochowa, Polska\", 224000),\n",
            "#     (\"Radom, Polska\", 214000),\n",
            "#     (\"Toruń, Polska\", 202074),\n",
            "#     (\"Kielce, Polska\", 200000),\n",
            "#     (\"Rzeszów, Polska\", 196000),\n",
            "#     (\"Opole, Polska\", 128000),\n",
            "#     (\"Gliwice, Polska\", 180000),\n",
            "#     (\"Zabrze, Polska\", 170000),\n",
            "#     (\"Elbląg, Polska\", 120000),\n",
            "#     (\"Płock, Polska\", 120000),\n",
            "#     (\"Nowy Sącz, Polska\", 85_000),\n",
            "#     (\"Słupsk, Polska\", 91_000),\n",
            "#     # (\"Świętochłowice, Polska\", 55_000),\n",
            "#     (\"Jelenia Góra, Polska\", 79_000),\n",
            "#     (\"Stalowa Wola, Polska\", 75_000),\n",
            "#     # (\"Koszalin, Polska\", 108_000),\n",
            "#     (\"Mielec, Polska\", 59_000),\n",
            "#     (\"Legnica, Polska\", 100_000),\n",
            "#     (\"Tychy, Polska\", 130_000),\n",
            "#     (\"Chorzów, Polska\", 110_000),\n",
            "#     # (\"Rybnik, Polska\", 140_000)\n",
            "# ]\n",
            "\n",
            "# Randomly select 8 cities\n",
            "random_cities = random.sample(cities, random_cities_count)\n",
            "\n",
            "loader = DataLoader(2, 5000, 5000, data_dir=data_dir)\n",
            "managers = []\n",
            "files = []\n",
            "\n",
            "for city, population in random_cities:\n",
            "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
            "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
            "    manager = loader.load_city_grid(city,file_name)\n",
            "    files.append(file_name)\n",
            "    managers.append(manager)\n",
            "    loader.add_elevation_to_grid(manager)\n",
            "    loader.add_residential_to_grid(manager)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "805aec81",
         "metadata": {},
         "source": [
            "##### 4. Model unet 256x256, grid density 2 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8d36ac63",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_256_2m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a00ee162",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "30fa874b",
         "metadata": {},
         "source": [
            "##### 5. Model unet 512x512, grid density 2 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a8d3ab38",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=512,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=128,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_512_2m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c8ef17b7",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6f1b8a2d",
         "metadata": {},
         "source": [
            "### Evaluation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fda74ab0",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "from grid_manager import GridManager\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\"),     # where to save the model\n",
            "    input_third_dimension=3,\n",
            "    output_third_dimension=2\n",
            ")\n",
            "\n",
            "# folder = Path(data_dir)\n",
            "# files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "# print(f\"files: {files}\")\n",
            "\n",
            "# trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "#     model=model,                # the model defined above\n",
            "#     files=files\n",
            "# )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2d6690ab",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np\n",
            "import time\n",
            "from trainer.model import PREDICT_GRID_INDICES\n",
            "\n",
            "\n",
            "grid_manager = GridManager(\"Bydgoszcz-Polska.city_grid\", data_dir=\"grids/with-is-residential\")\n",
            "metadata = grid_manager.get_metadata()\n",
            "\n",
            "assert metadata.third_dimension_size == 3, \"You have an outdated version of GridManager. Consider downloading grid files again to get all data...\"\n",
            "\n",
            "fragment_row, fragment_col = 2500, 2500\n",
            "fragment_height, fragment_width = 1500, 1500\n",
            "\n",
            "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
            "being_predicted = GridManager(\n",
            "    f\"test_{time.time()}.city_grid\",\n",
            "    fragment_height, fragment_width,\n",
            "    0,0,\n",
            "    metadata.grid_density,\n",
            "    segment_h, segment_w,\n",
            "    data_dir=\"grids/evaluation\",\n",
            "    third_dimension_size=metadata.third_dimension_size\n",
            ")\n",
            "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
            "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
            "print(f\"displayed: {displayed.shape}\")\n",
            "\n",
            "being_predicted.write_arbitrary_fragment(\n",
            "    grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width),\n",
            "    0, 0\n",
            ") # for instance some segment in the middle\n",
            "\n",
            "result = model.predict(being_predicted)\n",
            "img = result.read_arbitrary_fragment(\n",
            "    0, 0,\n",
            "    fragment_height - model.get_input_grid_surplus(),\n",
            "    fragment_width - model.get_input_grid_surplus()\n",
            ")[:, :, PREDICT_GRID_INDICES.IS_STREET]\n",
            "\n",
            "print(f\"DEBUG: img.max(): {img.max()}\")\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
            "plt.gray()\n",
            "plt.imshow(dilated)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ab1c4dfc",
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.imshow(img)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "35a9fce7",
         "metadata": {},
         "outputs": [],
         "source": [
            "fragment_row, fragment_col = 2000, 2000\n",
            "fragment_height, fragment_width = 3000, 3000\n",
            "\n",
            "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
            "\n",
            "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
            "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
            "print(f\"displayed: {displayed.shape}\")\n",
            "\n",
            "plt.gray()\n",
            "plt.imshow(displayed[:, :, 0], cmap=\"gray\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "d66d95c4",
         "metadata": {},
         "source": [
            "Version used by Jakub in \"only is_street\" dimension in input data model training tests - below:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1acaabae",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "from grid_manager import GridManager\n",
            "\n",
            "model = ClippingModel(\n",
            "    ClipModels.SHALLOWED_UNET,\n",
            "    clipping_size=256,\n",
            "    clipping_surplus=64,\n",
            "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\"),\n",
            "    input_third_dimension=1,\n",
            "    output_third_dimension=1\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "faa8f96d",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np\n",
            "import time\n",
            "from trainer.model import PREDICT_GRID_INDICES\n",
            "\n",
            "\n",
            "grid_manager = GridManager(\"Bydgoszcz-Polska.city_grid\", data_dir=\"grids/with-is-residential\")\n",
            "metadata = grid_manager.get_metadata()\n",
            "\n",
            "fragment_row, fragment_col = 2500, 2500\n",
            "fragment_height, fragment_width = 1500, 1500\n",
            "\n",
            "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
            "being_predicted = GridManager(\n",
            "    f\"test_{time.time()}.city_grid\",\n",
            "    fragment_height, fragment_width,\n",
            "    0,0,\n",
            "    metadata.grid_density,\n",
            "    segment_h, segment_w,\n",
            "    data_dir=\"grids/evaluation\",\n",
            "    third_dimension_size=1\n",
            ")\n",
            "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
            "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
            "print(f\"displayed: {displayed.shape}\")\n",
            "\n",
            "being_predicted.write_arbitrary_fragment(\n",
            "    grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width),\n",
            "    0, 0\n",
            ") # for instance some segment in the middle\n",
            "\n",
            "result = model.predict(being_predicted)\n",
            "img = result.read_arbitrary_fragment(\n",
            "    0, 0,\n",
            "    fragment_height - model.get_input_grid_surplus(),\n",
            "    fragment_width - model.get_input_grid_surplus()\n",
            ")[:, :, PREDICT_GRID_INDICES.IS_STREET]\n",
            "\n",
            "print(f\"DEBUG: img.max(): {img.max()}\")\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
            "plt.gray()\n",
            "plt.imshow(dilated)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "33d12be9",
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.imshow(img)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "af837657",
         "metadata": {},
         "outputs": [],
         "source": [
            "fragment_row, fragment_col = 2000, 2000\n",
            "fragment_height, fragment_width = 3000, 3000\n",
            "\n",
            "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
            "\n",
            "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
            "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
            "print(f\"displayed: {displayed.shape}\")\n",
            "\n",
            "plt.gray()\n",
            "plt.imshow(displayed[:, :, 0], cmap=\"gray\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "953f8fae",
         "metadata": {},
         "source": [
            "## Examples of modules usages\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3402120c",
         "metadata": {},
         "source": [
            "### Download mesh and get grid from OSM\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b0390497",
         "metadata": {},
         "outputs": [],
         "source": [
            "from scraper.data_loader import DataLoader\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2ec41555",
         "metadata": {},
         "outputs": [],
         "source": [
            "# IMPORTANT!! - dataloader checks, if the file already exists\n",
            "from scraper.data_loader import DataLoader\n",
            "\n",
            "cityname = \"Kraków\"\n",
            "\n",
            "loader = DataLoader(1)\n",
            "\n",
            "grid_manager = loader.load_city_grid(cityname, cityname + \".dat\")\n",
            "# loader.add_elevation_to_grid(grid_manager)\n",
            "loader.add_residential_to_grid(grid_manager)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a399ff60",
         "metadata": {},
         "source": [
            "#### Display downloaded grid (dilated to be better visible)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "adba9939",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np\n",
            "\n",
            "grid_manager = GridManager(\"Tychy.dat\", data_dir=\"grids\")\n",
            "img = grid_manager.read_segment(2,0)[:5000, :5000, 0]\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
            "plt.gray()\n",
            "plt.imshow(dilated)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "853958a1",
         "metadata": {},
         "source": [
            "#### Adding is residential tests\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "dbaa532c",
         "metadata": {},
         "outputs": [],
         "source": [
            "# IMPORTANT!! - dataloader checks, if the file already exists\n",
            "from scraper.data_loader import DataLoader\n",
            "\n",
            "cityname = \"Tychy\"\n",
            "\n",
            "loader = DataLoader(1)\n",
            "\n",
            "grid_manager = loader.load_city_grid(cityname, cityname + \".dat\")\n",
            "# loader.add_elevation_to_grid(grid_manager)\n",
            "loader.add_residential_to_grid(grid_manager)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a2fa9803",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "from scraper.rasterizer import Rasterizer \n",
            "import matplotlib.pyplot as plt\n",
            "import math\n",
            "\n",
            "grid_manager = GridManager(\"Toruń.dat\")\n",
            "meta = grid_manager.get_metadata()\n",
            "segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
            "segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
            "\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "fig, axes = plt.subplots(segments_rows, segments_cols)\n",
            "fig.set_size_inches((16, 16))\n",
            "axes = axes.flatten()\n",
            "axes_index = 0\n",
            "\n",
            "for row_idx in range(segments_rows):\n",
            "    for col_idx in range(segments_cols):\n",
            "        segment = grid_manager.read_segment(row_idx, col_idx)\n",
            "\n",
            "        axes[axes_index].imshow(segment[:, :, 2], cmap=\"gray\")\n",
            "        axes[axes_index].set_title(f\"Segment_rows: {row_idx}, segment_cols: {col_idx}\")\n",
            "        axes_index += 1"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "69284905",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "from scraper.rasterizer import Rasterizer \n",
            "import matplotlib.pyplot as plt\n",
            "import math\n",
            "\n",
            "grid_manager = GridManager(\"Toruń.dat\")\n",
            "meta = grid_manager.get_metadata()\n",
            "segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
            "segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
            "\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "fig, axes = plt.subplots(segments_rows, segments_cols)\n",
            "fig.set_size_inches((16, 16))\n",
            "axes = axes.flatten()\n",
            "axes_index = 0\n",
            "\n",
            "for row_idx in range(segments_rows):\n",
            "    for col_idx in range(segments_cols):\n",
            "        segment = grid_manager.read_segment(row_idx, col_idx)\n",
            "\n",
            "        axes[axes_index].imshow(segment[:, :, 0], cmap=\"gray\")\n",
            "        axes[axes_index].set_title(f\"Segment_rows: {row_idx}, segment_cols: {col_idx}\")\n",
            "        axes_index += 1\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c3df3944",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "\n",
            "grid_manager = GridManager(\"Tychy.dat\")\n",
            "img = grid_manager.read_segment(1,1)[:2000, :2000, 0]\n",
            "\n",
            "fig, axs = plt.subplots(2, 2, figsize=(10,5))\n",
            "fig.suptitle(\"Obrazki dróg (zdylatowane, żeby było lepiej widać)\")\n",
            "\n",
            "imgs = [\n",
            "    [grid_manager.read_segment(0,0)[3000:, 3000:, 0], grid_manager.read_segment(0,1)[3000:, :2000, 0]],\n",
            "    [grid_manager.read_segment(1,0)[:2000, 3000:, 0], grid_manager.read_segment(1,1)[:2000, :2000, 0]]\n",
            "]\n",
            "\n",
            "dilated_imgs = []\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "\n",
            "for imgs_row in imgs:\n",
            "    target_row = []\n",
            "    dilated_imgs.append(target_row)\n",
            "    for img in imgs_row:\n",
            "        target_row.append(cv2.dilate(img, struct_el, iterations=3))\n",
            "\n",
            "plt.gray()\n",
            "axs[0,0].imshow(dilated_imgs[0][0])\n",
            "axs[0,1].imshow(dilated_imgs[0][1])\n",
            "axs[1,0].imshow(dilated_imgs[1][0])\n",
            "axs[1,1].imshow(dilated_imgs[1][1])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "863446e5",
         "metadata": {},
         "outputs": [],
         "source": [
            "from scraper.grid_builder import GridBuilder\n",
            "import math\n",
            "from scraper.rasterizer import Rasterizer\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "city = \"Tychy, Polska\"\n",
            "builder = GridBuilder()\n",
            "\n",
            "gdf_edges = builder.get_city_roads(city)\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "\n",
            "grid_2d = rasterizer.get_rasterize_roads(gdf_edges, 1, is_residential=False)\n",
            "plt.imshow(grid_2d, cmap=\"gray\")\n",
            "plt.title(f\"City: {city}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "af730340",
         "metadata": {},
         "outputs": [],
         "source": [
            "from scraper.grid_builder import GridBuilder\n",
            "import math\n",
            "from scraper.rasterizer import Rasterizer\n",
            "import matplotlib.pyplot as plt\n",
            "import geopandas as gpd\n",
            "\n",
            "city = \"Kraków\"\n",
            "builder = GridBuilder()\n",
            "grid_density = 1\n",
            "segment_h = 5000\n",
            "segment_w = 5000\n",
            "data_dir = \"grids\"\n",
            "\n",
            "gdf_edges = builder.get_city_roads(city)\n",
            "min_x, min_y, max_x, max_y = gdf_edges.total_bounds\n",
            "\n",
            "columns_number = math.ceil((max_x - min_x) / grid_density)\n",
            "rows_number = math.ceil((max_y - min_y) / grid_density)\n",
            "\n",
            "segment_rows = math.ceil((rows_number) / segment_h)\n",
            "segment_cols = math.ceil((columns_number) / segment_h)\n",
            "\n",
            "# grid_manager = GridManager(file_name, rows_number=int(rows_number), columns_number=int(columns_number),\n",
            "#                             grid_density=grid_density, segment_h=segment_h, segment_w=segment_w,\n",
            "#                             data_dir=data_dir, upper_left_longitude=min_x, upper_left_latitude=max_y)\n",
            "print(f\"Height: {int(rows_number)}, Width: {int(columns_number)}, rows: {segment_rows}, cols: {segment_cols}\")\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "fig, axes = plt.subplots(segment_rows, segment_cols)\n",
            "fig.set_size_inches((16, 16))\n",
            "axes = axes.flatten()\n",
            "axes_index = 0\n",
            "# single_row_gdf = gdf_edges.sample(1)\n",
            "\n",
            "# grid_2d = rasterizer.rasterize_segment_from_indexes(gdf_edges=single_row_gdf, indexes=(0, 0), is_residential=False\n",
            "#                                                             size_h=segment_h, size_w=segment_w,\n",
            "#                                                             pixel_size=grid_density)\n",
            "# print(grid_2d.shape)\n",
            "# plt.imshow(grid_2d, cmap=\"gray\")\n",
            "\n",
            "for i in range(segment_rows):\n",
            "    for j in range(segment_cols):\n",
            "        grid_2d = rasterizer.rasterize_segment_from_indexes(gdf_edges=gdf_edges, indexes=(i, j), is_residential=False,\n",
            "                                                            size_h=segment_h, size_w=segment_w,\n",
            "                                                            pixel_size=grid_density)\n",
            "        axes[axes_index].imshow(grid_2d, cmap=\"gray\")\n",
            "        axes[axes_index].set_title(f\"Segment_rows: {i}, segment_cols: {j}\")\n",
            "        axes_index += 1"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a3f8acb1",
         "metadata": {},
         "source": [
            "### Data management\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1de7261b",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "import numpy as np\n",
            "\n",
            "filename = \"przyklad1.dat\"\n",
            "\n",
            "src_man = GridManager(filename, 2000, 2000, 0.0, 0.0, 1, 3, 3, data_dir=\"grids\")\n",
            "\n",
            "# a = np.zeros((2000, 2000, 2), dtype=np.float64)\n",
            "is_street = np.array([\n",
            "    [1, 2, 3, 4, 5, 6],\n",
            "    [11, 12, 13, 14, 15, 16],\n",
            "    [21, 22, 23, 24, 25, 26],\n",
            "    [31, 32, 33, 34, 35, 36],\n",
            "    [41, 42, 43, 44, 45, 46],\n",
            "    [51, 52, 53, 54, 55, 56],\n",
            "])\n",
            "a = np.zeros((is_street.shape[0], is_street.shape[1], 3))\n",
            "a[:,:, 0] = is_street\n",
            "\n",
            "for x in range(2):\n",
            "    for y in range(2):\n",
            "        src_man.write_segment(a[y * 3: (y + 1) * 3, x*3:(x + 1) * 3], y, x)\n",
            "\n",
            "\n",
            "src_man.write_segment(a[:2,:3, :] + 1, 666, 0)\n",
            "src_man.write_segment(a[:3,:2, :] + 2, 0, 666)\n",
            "\n",
            "man = src_man.deep_copy()\n",
            "\n",
            "print(f\"1: {man.read_segment(0, 0)[:, :, 0]}\")\n",
            "print(f\"2: {man.read_segment(0, 1)[:, :, 0]}\")\n",
            "print(f\"3: {man.read_segment(1, 0)[:, :, 0]}\")\n",
            "print(f\"4: {man.read_segment(1, 1)[:, :, 0]}\")\n",
            "print(f\"5: {man.read_segment(666, 666)[:, :, 0]}\")\n",
            "\n",
            "print(man.get_metadata())\n",
            "\n",
            "man.delete()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c1a46640",
         "metadata": {},
         "source": [
            "### Streets discovery\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0770ae13",
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def show_crossroads(img_height, img_width, conflictless_crossroads, conflicting_crossroads):\n",
            "    fig, axs = plt.subplots(  max(2, len(conflictless_crossroads), len(conflicting_crossroads)), 6, figsize=(10,5))\n",
            "    axs[0, 0].set_title(\"conflictless\")\n",
            "    axs[0, 1].set_title(\"junctions\")\n",
            "    axs[0, 2].set_title(\"conflicts\")\n",
            "    axs[0, 3].set_title(\"conflicting\")\n",
            "    axs[0, 4].set_title(\"junctions\")\n",
            "    axs[0, 5].set_title(\"conflicts\")\n",
            "\n",
            "\n",
            "    for i in range(len(conflictless_crossroads)):\n",
            "        crossroad = conflictless_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 0].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflictless_crossroads)):\n",
            "        crossroad = conflictless_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for street in crossroad.street_junctions.keys():\n",
            "            crossroad_image[crossroad.street_junctions[street]] = 1\n",
            "\n",
            "        axs[i, 1].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflictless_crossroads)):\n",
            "        crossroad = conflictless_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.conflicting_points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 2].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflicting_crossroads)):\n",
            "        crossroad = conflicting_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 3].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflicting_crossroads)):\n",
            "        crossroad = conflicting_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for street in crossroad.street_junctions.keys():\n",
            "            crossroad_image[crossroad.street_junctions[street]] = 1\n",
            "\n",
            "        axs[i, 4].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflicting_crossroads)):\n",
            "        crossroad = conflicting_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.conflicting_points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 5].imshow(crossroad_image)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "131fd71e",
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def show_streets(img_height, img_width, conflictless_streets, conflicting_streets):\n",
            "    fig, axs = plt.subplots(  max(2, len(conflictless_streets), len(conflicting_streets)), 4, figsize=(10,8))\n",
            "    axs[0, 0].set_title(\"conflictless\")\n",
            "    axs[0, 1].set_title(\"conflicts\")\n",
            "    axs[0, 2].set_title(\"conflicting\")\n",
            "    axs[0, 3].set_title(\"conflicts\")\n",
            "\n",
            "\n",
            "    for i in range(len(conflictless_streets)):\n",
            "        street = conflictless_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.linestring:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 0].imshow(street_image)\n",
            "    \n",
            "    for i in range(len(conflictless_streets)):\n",
            "        street = conflictless_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.conflicts:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 1].imshow(street_image)\n",
            "\n",
            "    for i in range(len(conflicting_streets)):\n",
            "        street = conflicting_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.linestring:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 2].imshow(street_image)\n",
            "    \n",
            "    for i in range(len(conflicting_streets)):\n",
            "        street = conflicting_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.conflicts:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 3].imshow(street_image)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "25f547aa",
         "metadata": {},
         "source": [
            "#### Diamond\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7fc0609f",
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "from skimage.morphology import skeletonize\n",
            "from graph_remaker.morphological_remaker import discover_streets\n",
            "import cv2\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "image = cv2.imread(\"test_images/diamond.png\", cv2.IMREAD_GRAYSCALE)\n",
            "plt.gray()\n",
            "bin_image = image > 0\n",
            "# plt.imshow(bin_image * 255)\n",
            "\n",
            "processed_image = bin_image[:, :, np.newaxis]\n",
            "processed_image = np.concatenate((processed_image, np.zeros(processed_image.shape)), axis=2)\n",
            "\n",
            "conflictless_crossroads, conflicting_crossroads, conflictless_streets, conflicting_streets = discover_streets(processed_image)\n",
            "\n",
            "height, width = image.shape\n",
            "plt.imshow(image)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e87e4f55",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_crossroads(height, width, conflictless_crossroads, conflicting_crossroads)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a7a21635",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_streets(height, width, conflictless_streets, conflicting_streets)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "7b0960ab",
         "metadata": {},
         "source": [
            "#### Conflicting crossroad and dead-end\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9d3b7375",
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "from skimage.morphology import skeletonize\n",
            "from graph_remaker.morphological_remaker import discover_streets\n",
            "import cv2\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "image = cv2.imread(\"test_images/crossroad_conflict_deadend.png\", cv2.IMREAD_GRAYSCALE)\n",
            "plt.gray()\n",
            "bin_image = image > 0\n",
            "# plt.imshow(bin_image * 255)\n",
            "\n",
            "processed_image = bin_image[:, :, np.newaxis]\n",
            "processed_image = np.concatenate((processed_image, np.zeros(processed_image.shape)), axis=2)\n",
            "\n",
            "conflictless_crossroads, conflicting_crossroads, conflictless_streets, conflicting_streets = discover_streets(processed_image)\n",
            "\n",
            "height, width = image.shape\n",
            "plt.imshow(image)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3b7ed360",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_crossroads(width, height, conflictless_crossroads, conflicting_crossroads)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c63a915d",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_streets(width, height, conflictless_streets, conflicting_streets)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8e175b48",
         "metadata": {},
         "source": [
            "### Model training (tutorial, not used in practice anymore. For practical use, look at the higher entries in the notebook)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a11c66cd",
         "metadata": {},
         "source": [
            "##### Use the whole VRAM\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b2ee2324",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import tf\n",
            "\n",
            "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
            "if gpus:\n",
            "    try:\n",
            "        for gpu in gpus:\n",
            "            tf.config.experimental.set_memory_growth(gpu, False)\n",
            "        tf.config.set_logical_device_configuration(\n",
            "            gpus[0],\n",
            "            [tf.config.LogicalDeviceConfiguration(memory_limit=4000)] # Limit to 4000 MB of VRAM - TODO: adjust based on your GPU\n",
            "        )\n",
            "    except RuntimeError as e:\n",
            "        print(e)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e6e2a0e3",
         "metadata": {},
         "source": [
            "##### Create batch sequence of clippings that are the candidates for a model and print the shape of the input\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d51faed8",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_sequence import ClippingBatchSequence # not used anymore in the project - only for visualisation\n",
            "from trainer.batch_sequence import BatchSequence            # not used anymore in the project - only for visualisation\n",
            "\n",
            "batchSeq = ClippingBatchSequence(           # Define a clipping batch sequence\n",
            "            BatchSequence(                      # Define a batch sequence\n",
            "                files=list([\"Tychy.dat\"]),          # list of files to use\n",
            "                batch_size=1,                       # not important here\n",
            "                cut_sizes=[(512, 512)],             # sizes of cuttings to make from the grid\n",
            "            ),\n",
            "            clipping_size=512,                  # size of the clipping (input to the model)\n",
            "            input_grid_surplus=64,              # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "        )\n",
            "\n",
            "X, Y = batchSeq[0]          # get the first batch\n",
            "print(\"Shape of the input batch: \", X.shape)              # print the shape of the input batch"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2ce9534c",
         "metadata": {},
         "source": [
            "##### Display example clipping (X and y) - `WARNING: you may need to run the above code up to approx. 3-5 times in order to get an image below, other than a void one`\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3e4ae7aa",
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
            "\n",
            "axs[0].imshow(X[0, :, :, 0], cmap='grey')\n",
            "axs[1].imshow(Y[\"is_street\"][0, :, :, 0], cmap='grey')\n",
            "\n",
            "print(\"X min/max:\", X[0, :, :, 0].min(), X[0, :, :, 0].max())\n",
            "print(\"Y min/max:\", Y[\"is_street\"][0, :, :, 0].min(), Y[\"is_street\"][0, :, :, 0].max())"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ba1a9a19",
         "metadata": {},
         "source": [
            "##### Show available GPUs\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "20aabc2d",
         "metadata": {},
         "outputs": [],
         "source": [
            "from tensorflow.python.client import device_lib\n",
            "import tensorflow as tf\n",
            "\n",
            "def get_available_devices():\n",
            "    local_device_protos = device_lib.list_local_devices()\n",
            "    return [x.name for x in local_device_protos]\n",
            "\n",
            "print(get_available_devices())\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "11892b0d",
         "metadata": {},
         "source": [
            "##### Define a model and then cities for the trainer (e.g. Tychy) as the files list\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d17e40c5",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels       # Import ClipModels Enum\n",
            "\n",
            "print(\"Possible models:\")\n",
            "for model in ClipModels:\n",
            "    print(f\"- {model.name}: {model.value}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "6716a8f5",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.BASE,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=512,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=\"model_test_16_01\"     # where to save the model\n",
            ")\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=[                     # list of data files to use for training\n",
            "        \"Tychy.dat\",\n",
            "        # \"Kraków.dat\"\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f8c5b859",
         "metadata": {},
         "source": [
            "##### Run fitting which created the above batch sequence and trains the model\n",
            "\n",
            "`random_fit_from_files(self, epochs: int = 100, steps_per_epoch=1000, batch_size=32)`\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "30bd51e2",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=10,             # number of steps per epoch\n",
            "    batch_size=32                   # size of each training batch\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "151c07e9",
         "metadata": {},
         "source": [
            "##### Other models etc.\n",
            "\n",
            "You may need to run \"Use the whole VRAM\" cell before them (separately and probably after the kernel restart)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b880c096",
         "metadata": {},
         "source": [
            "#### UNET\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b222d447",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
            "\n",
            "model = ClippingModel(\n",
            "    ClipModels.UNET,\n",
            "    clipping_size=512,\n",
            "    clipping_surplus=64,\n",
            "    path=\"model_test_13_01_13_22\"\n",
            ")\n",
            "trainer = Trainer(\n",
            "    model=model,\n",
            "    files=[\n",
            "        \"Tychy.dat\",\n",
            "        \"Kraków.dat\"\n",
            "    ]\n",
            ")\n",
            "# Na mojej laptopowej RTX 3050 Ti 4GB, batch_size=8 nie poradził sobie z pamięcią po 4 minutach.\n",
            "# Zmieniłem batch_size na 2 i działa - 44 sekundy na poniższym przykładzie\n",
            "# UPDATE: Zmieniłem na 4, działa dobrze\n",
            "# - trochę ponad minuta po ustawieniu (w wyższej komórce) memory_limit=4000\n",
            "trainer.random_fit_from_files(1, 10, batch_size=4)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b5f82b17",
         "metadata": {},
         "source": [
            "#### AlexInspired\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "dd0da5d8",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
            "\n",
            "model = ClippingModel(\n",
            "    ClipModels.ALEX_INSPIRED,\n",
            "    clipping_size=256,\n",
            "    clipping_surplus=32,\n",
            "    path=\"model_test_13_01_13_22\"\n",
            ")\n",
            "trainer = Trainer(\n",
            "    model=model,\n",
            "    files=[\n",
            "        \"Tychy.dat\",\n",
            "        \"Kraków.dat\"\n",
            "    ]\n",
            ")\n",
            "\n",
            "# u mnie zajęło 41 minut i 1.1 sekundy\n",
            "trainer.random_fit_from_files(1, 10, batch_size=4)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a864c4df36b3e3e0",
         "metadata": {},
         "source": [
            "### upper algorithm\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3ceac6721d76834b",
         "metadata": {
            "ExecuteTime": {
               "end_time": "2026-01-13T20:22:07.222397Z",
               "start_time": "2026-01-13T20:21:50.479488Z"
            }
         },
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import networkx as nx\n",
            "import os\n",
            "import cv2\n",
            "import numpy as np\n",
            "import math\n",
            "from grid_manager import GridManager, GRID_INDICES\n",
            "from graph_remaker.memory_wise import process_large_grid\n",
            "\n",
            "# 1. GENEROWANIE SZTUCZNEGO OBRAZU\n",
            "H, W = 1000, 1000\n",
            "SEG_SIZE = 200\n",
            "TEMP_GRID_FILE = \"square_test.dat\"\n",
            "\n",
            "\n",
            "print(f\"Generowanie obrazu {W}x{H}...\")\n",
            "img = np.zeros((H, W), dtype=np.uint8)\n",
            "\n",
            "# Rysujemy KWADRAT (linie prostopadłe do osi)\n",
            "# Współrzędne: (150, 150) do (850, 850)\n",
            "# Linie będą przecinać granice segmentów (które są co 200px) idealnie prosto.\n",
            "cv2.rectangle(img, (150, 150), (850, 850), 255, thickness=10)\n",
            "\n",
            "# Dodajmy Krzyż w środku (żeby przetestować skrzyżowania typu \"4-way\")\n",
            "cv2.line(img, (500, 150), (500, 850), 255, thickness=10) # Pion\n",
            "cv2.line(img, (150, 500), (850, 500), 255, thickness=10) # Poziom\n",
            "\n",
            "print(\"Obraz wygenerowany.\")\n",
            "\n",
            "# 2. ZAPIS DO PLIKU\n",
            "if os.path.exists(os.path.join(\"grids\", TEMP_GRID_FILE)):\n",
            "    os.remove(os.path.join(\"grids\", TEMP_GRID_FILE))\n",
            "\n",
            "gm = GridManager(TEMP_GRID_FILE, H, W,\n",
            "                 upper_left_longitude=0.0, upper_left_latitude=0.0,\n",
            "                 grid_density=1.0,\n",
            "                 segment_h=SEG_SIZE, segment_w=SEG_SIZE)\n",
            "\n",
            "img_binary = (img > 0).astype(np.float32)\n",
            "\n",
            "rows_n = math.ceil(H / SEG_SIZE)\n",
            "cols_n = math.ceil(W / SEG_SIZE)\n",
            "\n",
            "print(f\"Dzielenie na {rows_n}x{cols_n} segmentów...\")\n",
            "\n",
            "for r in range(rows_n):\n",
            "    for c in range(cols_n):\n",
            "        y_start = r * SEG_SIZE\n",
            "        x_start = c * SEG_SIZE\n",
            "        y_end = min(y_start + SEG_SIZE, H)\n",
            "        x_end = min(x_start + SEG_SIZE, W)\n",
            "\n",
            "        chunk = img_binary[y_start:y_end, x_start:x_end]\n",
            "        gm.write_segment(chunk, r, c)\n",
            "\n",
            "print(\"Dane zapisane. Uruchamiam memory_wise...\")\n",
            "\n",
            "# 3. URUCHOMIENIE\n",
            "try:\n",
            "    # Używamy tej wersji memory_wise, którą masz teraz (z patchem i snappingiem)\n",
            "    graph = process_large_grid(TEMP_GRID_FILE)\n",
            "\n",
            "    # 4. WIZUALIZACJA\n",
            "    print(f\"\\n--- WYNIK ---\")\n",
            "    print(f\"Węzły: {len(graph.nodes)}\")\n",
            "    print(f\"Krawędzie: {len(graph.edges)}\")\n",
            "\n",
            "    plt.figure(figsize=(12, 6))\n",
            "\n",
            "    # Lewa: Obraz\n",
            "    plt.subplot(1, 2, 1)\n",
            "    plt.title(f\"Idealny Kwadrat (Linie proste)\")\n",
            "    plt.imshow(img, cmap='gray')\n",
            "\n",
            "    # Siatka cięcia\n",
            "    for y in range(0, H, SEG_SIZE):\n",
            "        plt.axhline(y, color='yellow', linestyle='--', alpha=0.3)\n",
            "    for x in range(0, W, SEG_SIZE):\n",
            "        plt.axvline(x, color='yellow', linestyle='--', alpha=0.3)\n",
            "\n",
            "    # Prawa: Graf\n",
            "    plt.subplot(1, 2, 2)\n",
            "    plt.title(\"Zrekonstruowany Graf\")\n",
            "\n",
            "    if len(graph.nodes) > 0:\n",
            "        pos = {n: (data['x'], data['y']) for n, data in graph.nodes(data=True)}\n",
            "\n",
            "        nx.draw(graph, pos,\n",
            "                node_size=20,\n",
            "                node_color='red',\n",
            "                edge_color='cyan',\n",
            "                width=2,\n",
            "                with_labels=False,\n",
            "                arrows=False)\n",
            "        plt.axis('equal')\n",
            "        plt.gca().invert_yaxis() # Żeby góra była na górze\n",
            "    else:\n",
            "        plt.text(0.5, 0.5, \"BRAK WYNIKÓW\", ha='center', color='red', fontsize=15)\n",
            "\n",
            "    plt.tight_layout()\n",
            "    plt.show()\n",
            "\n",
            "except Exception as e:\n",
            "    print(f\"CRITICAL ERROR: {e}\")\n",
            "    import traceback\n",
            "    traceback.print_exc()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "croada",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
