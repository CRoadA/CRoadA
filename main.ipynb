{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7133f1d",
   "metadata": {},
   "source": [
    "## Training target model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c05d3",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce8ff6",
   "metadata": {},
   "source": [
    "1. Pull the latest version of repository - to see, which models were already chosen by others.\n",
    "2. Install new `requirements.txt`. It should work with python 3.12 and upgraded pip. It contains cuda tensrflow version. If you have problems with installation, try with `requirements-no-gpu.txt`.\n",
    "3. Check, if tensorflow sees your graphics card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba232e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 19:12:25.071876: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-06 19:12:25.113587: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-06 19:12:26.065323: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/jakseluz/miniconda3/envs/croada/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source - https://stackoverflow.com/a\n",
    "# Posted by Wilmar van Ommeren, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2026-01-18, License - CC BY-SA 4.0\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9dafc6",
   "metadata": {},
   "source": [
    "it should return something like:\n",
    "\n",
    "```\n",
    "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "```\n",
    "\n",
    "4. Running model involves preparing GPU, downloading data and running appropriate training. Everything is described below.\n",
    "5. And don't forget to push your name next to chosen model to repository!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ce42f",
   "metadata": {},
   "source": [
    "#### Use the whole VRAM\n",
    "\n",
    "Set VRAM size accoring to your GPU!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c176fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_model import tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=4000)] # Limit to 4000 MB of VRAM - TODO: adjust based on your GPU\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5dc5f",
   "metadata": {},
   "source": [
    "### Loading data and training model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186010ba",
   "metadata": {},
   "source": [
    "Model training subsections are divided by grid density. Choose one model training subsection and then please clear cell outputs, **ADD your name in chosen model section header** and push this change on the repository. This should help us avoid conflicts, that 2 people will choose the same model.\n",
    "\n",
    "So the header sholud like this:\n",
    "\n",
    "```markdown\n",
    "##### **GRZEGORZ** 1. Model - shallowed unet 256x256, grid density 1 m\n",
    "```\n",
    "\n",
    "#### trainer.random_fit_from_files() arguments\n",
    "\n",
    "At the moment `epoch_steps` and `epochs` are set to 1. Run it and check, how long does it take to execute it. Then adjust these values to the amout of time you have. `epoch_steps` over 200 probably doesn't make sense, so later on increase just `epochs`.\n",
    "\n",
    "If it lasts over 15 minutes, it means that probably there is some problem with graphics card. Nevertheless, it should run properly. Model is dumped to a different the file after each training and starts from the last checkpoint everytime, so don't hesitate to run it multiple times, if you want/have time to do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f87fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(\"grids\", \"with-is-residential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2779acc",
   "metadata": {},
   "source": [
    "#### Grid density 1m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56067167",
   "metadata": {},
   "source": [
    "##### Downloading cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c35b1",
   "metadata": {},
   "source": [
    "If you have limited disk space, you can adjust `random_cities_count` variable, to download less city grids. It will save time and disk space, but will probably affect model training.\n",
    "If you do this, please write somewhere down, which cities where used to train model - it will help when writing report.\n",
    "\n",
    "> **Note:** It lasts for a while - for Częstochowa it is up to 20 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09380f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scraper.data_loader import DataLoader\n",
    "from trainer.model import Model\n",
    "from trainer.trainer import Trainer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "random_cities_count = 11\n",
    "\n",
    "folder_path = Path(data_dir)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prefered cities to download\n",
    "cities = [\n",
    "    (\"Gdańsk, Polska\", 466630),\n",
    "    (\"Bydgoszcz, Polska\", 358928),\n",
    "    (\"Lublin, Polska\", 339433),\n",
    "    (\"Gdynia, Polska\", 246635),\n",
    "    (\"Radom, Polska\", 214000),\n",
    "    (\"Toruń, Polska\", 202074),\n",
    "    (\"Świętochłowice, Polska\", 55_000),\n",
    "    (\"Koszalin, Polska\", 108_000),\n",
    "    (\"Mielec, Polska\", 59_000),\n",
    "    (\"Chorzów, Polska\", 110_000),\n",
    "    (\"Rybnik, Polska\", 140_000)\n",
    "]\n",
    "\n",
    "# List of Polish cities with population between 50,000 and 500,000\n",
    "# cities = [\n",
    "#     # (\"Warsaw, Polska\", 1790658),\n",
    "#     # (\"Kraków, Polska\", 780981),\n",
    "#     # (\"Łódź, Polska\", 687702),\n",
    "#     # (\"Wrocław, Polska\", 640648),\n",
    "#     # (\"Poznań, Polska\", 538633),\n",
    "#     # (\"Gdańsk, Polska\", 466630),\n",
    "#     (\"Szczecin, Polska\", 402465),\n",
    "#     # (\"Bydgoszcz, Polska\", 358928),\n",
    "#     (\"Lublin, Polska\", 339433),\n",
    "#     (\"Białystok, Polska\", 297459),\n",
    "#     (\"Katowice, Polska\", 294510),\n",
    "#     # (\"Gdynia, Polska\", 246635),\n",
    "#     (\"Częstochowa, Polska\", 224000),\n",
    "#     (\"Radom, Polska\", 214000),\n",
    "#     (\"Toruń, Polska\", 202074),\n",
    "#     (\"Kielce, Polska\", 200000),\n",
    "#     (\"Rzeszów, Polska\", 196000),\n",
    "#     (\"Opole, Polska\", 128000),\n",
    "#     (\"Gliwice, Polska\", 180000),\n",
    "#     (\"Zabrze, Polska\", 170000),\n",
    "#     (\"Elbląg, Polska\", 120000),\n",
    "#     (\"Płock, Polska\", 120000),\n",
    "#     (\"Nowy Sącz, Polska\", 85_000),\n",
    "#     (\"Słupsk, Polska\", 91_000),\n",
    "#     # (\"Świętochłowice, Polska\", 55_000),\n",
    "#     (\"Jelenia Góra, Polska\", 79_000),\n",
    "#     (\"Stalowa Wola, Polska\", 75_000),\n",
    "#     # (\"Koszalin, Polska\", 108_000),\n",
    "#     (\"Mielec, Polska\", 59_000),\n",
    "#     (\"Legnica, Polska\", 100_000),\n",
    "#     (\"Tychy, Polska\", 130_000),\n",
    "#     (\"Chorzów, Polska\", 110_000),\n",
    "#     # (\"Rybnik, Polska\", 140_000)\n",
    "# ]\n",
    "\n",
    "# Randomly select 8 cities\n",
    "random_cities = random.sample(cities, random_cities_count)\n",
    "\n",
    "loader = DataLoader(1, 5000, 5000, data_dir=data_dir, input_third_dimension=3)\n",
    "managers = []\n",
    "files = []\n",
    "\n",
    "for city, population in random_cities:\n",
    "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
    "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
    "    manager = loader.load_city_grid(city,file_name)\n",
    "    files.append(file_name)\n",
    "    managers.append(manager)\n",
    "    loader.add_elevation_to_grid(manager)\n",
    "    loader.add_residential_to_grid(manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c3063",
   "metadata": {},
   "source": [
    "Version used by Jakub in \"only is_street\" dimension in input data model training tests - below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scraper.data_loader import DataLoader\n",
    "from trainer.model import Model\n",
    "from trainer.trainer import Trainer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "random_cities_count = 11\n",
    "\n",
    "folder_path = Path(data_dir)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prefered cities to download\n",
    "cities = [\n",
    "    (\"Gdańsk, Polska\", 466630),\n",
    "    (\"Bydgoszcz, Polska\", 358928),\n",
    "    (\"Lublin, Polska\", 339433),\n",
    "    (\"Gdynia, Polska\", 246635),\n",
    "    (\"Radom, Polska\", 214000),\n",
    "    (\"Toruń, Polska\", 202074),\n",
    "    (\"Świętochłowice, Polska\", 55_000),\n",
    "    (\"Koszalin, Polska\", 108_000),\n",
    "    (\"Mielec, Polska\", 59_000),\n",
    "    (\"Chorzów, Polska\", 110_000),\n",
    "    (\"Rybnik, Polska\", 140_000)\n",
    "]\n",
    "\n",
    "\n",
    "random_cities = random.sample(cities, random_cities_count)\n",
    "\n",
    "loader = DataLoader(1, 5000, 5000, data_dir=data_dir, input_third_dimension=3) # not =1 because it does not work then\n",
    "managers = []\n",
    "files = []\n",
    "\n",
    "for city, population in random_cities:\n",
    "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
    "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
    "    manager = loader.load_city_grid(city,file_name)\n",
    "    files.append(file_name)\n",
    "    managers.append(manager)\n",
    "    #loader.add_elevation_to_grid(manager)\n",
    "    #loader.add_residential_to_grid(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from grid_manager import GridManager\n",
    "\n",
    "\n",
    "def add_zero_dimension_third_channel(grid_manager: GridManager, dimension_index: int):\n",
    "    meta = grid_manager.get_metadata()\n",
    "\n",
    "    segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
    "    segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
    "\n",
    "    for row_idx in range(segments_rows):\n",
    "        for col_idx in range(segments_cols):\n",
    "            segment = grid_manager.read_segment(row_idx, col_idx)\n",
    "\n",
    "            h, w, _ = segment.shape\n",
    "\n",
    "            for y in range(h):\n",
    "                for x in range(w):\n",
    "                    segment[y, x, dimension_index] = 0\n",
    "\n",
    "            grid_manager.write_segment(segment, row_idx, col_idx)\n",
    "\n",
    "            print(f\"Segment [{row_idx}, {col_idx}] saved. Max value in dimension {dimension_index}: {np.max(segment[:, :, dimension_index]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random_cities_count = 11\n",
    "\n",
    "folder = Path(data_dir)\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "cities = [\n",
    "    (\"Gdańsk, Polska\", 466630),\n",
    "    (\"Bydgoszcz, Polska\", 358928),\n",
    "    (\"Lublin, Polska\", 339433),\n",
    "    (\"Gdynia, Polska\", 246635),\n",
    "    (\"Radom, Polska\", 214000),\n",
    "    (\"Toruń, Polska\", 202074),\n",
    "    (\"Świętochłowice, Polska\", 55_000),\n",
    "    (\"Koszalin, Polska\", 108_000),\n",
    "    (\"Mielec, Polska\", 59_000),\n",
    "    (\"Chorzów, Polska\", 110_000),\n",
    "    (\"Rybnik, Polska\", 140_000)\n",
    "]\n",
    "\n",
    "random_cities = random.sample(cities, random_cities_count)\n",
    "for city, population in random_cities:\n",
    "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
    "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
    "    grid_manager = GridManager(file_name, data_dir=data_dir)\n",
    "    add_zero_dimension_third_channel(grid_manager, dimension_index=1)\n",
    "    add_zero_dimension_third_channel(grid_manager, dimension_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e32d6",
   "metadata": {},
   "source": [
    "##### **GRZEGORZ** 1.1. Model - shallowed unet 256x256, grid density 1 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "from grid_manager import GridManager\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=256,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\"),     # where to save the model\n",
    "\n",
    "    input_third_dimension=4,\n",
    "    output_third_dimension=3\n",
    ")\n",
    "\n",
    "folder = Path(data_dir)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=1,                       # number of epochs to train\n",
    "    steps_per_epoch=50,             # number of steps per epoch\n",
    "    batch_size=8                   # size of each training batch\n",
    ")\n",
    "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252fecb",
   "metadata": {},
   "source": [
    "##### **JAKUB** 1.2. Model - shallowed unet 256x256, grid density 1 m - **no altitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fbc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770401563.491741   30015 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1770401563.509182   30015 cuda_executor.cc:508] failed to allocate 3.91GiB (4194304000 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: [<grid_manager.GridManager object at 0x7f5712c63fe0>, <grid_manager.GridManager object at 0x7f5712c61730>, <grid_manager.GridManager object at 0x7f5712c63fb0>, <grid_manager.GridManager object at 0x7f5712c61700>, <grid_manager.GridManager object at 0x7f571ab10a10>, <grid_manager.GridManager object at 0x7f5712cf02f0>, <grid_manager.GridManager object at 0x7f5712cf04d0>, <grid_manager.GridManager object at 0x7f5712cf0560>, <grid_manager.GridManager object at 0x7f5712cf05f0>, <grid_manager.GridManager object at 0x7f5712cf0680>, <grid_manager.GridManager object at 0x7f5712cf0710>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "from grid_manager import GridManager\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    model_type=ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=256,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"shallowed_unet_256_1m_is_street_only\"),     # where to save the model\n",
    "\n",
    "    input_third_dimension=2,\n",
    "    output_third_dimension=1\n",
    ")\n",
    "\n",
    "folder = Path(data_dir)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317cc6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 19:12:50.769845: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f55fc01ab70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-06 19:12:50.769858: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2026-02-06 19:12:51.015226: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2026-02-06 19:12:51.854009: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91800\n",
      "2026-02-06 19:13:03.881454: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=4} for conv (f16[64,3,3,192]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,128,128,192]{3,2,1,0}, f16[8,128,128,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2026-02-06 19:13:05.683836: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.802453331s\n",
      "Trying algorithm eng19{k2=4} for conv (f16[64,3,3,192]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,128,128,192]{3,2,1,0}, f16[8,128,128,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/100\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40:31\u001b[0m 25s/step - accuracy: 0.9879 - loss: 6.7493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1770401590.702887   30254 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9848 - loss: 0.9695 - val_accuracy: 0.9885 - val_loss: 0.7718\n",
      "Epoch 2/2\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 2s/step - accuracy: 0.9858 - loss: 0.7250 - val_accuracy: 0.9820 - val_loss: 0.9074\n"
     ]
    }
   ],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=2,                       # number of epochs to train\n",
    "    steps_per_epoch=100,             # number of steps per epoch\n",
    "    batch_size=8                   # size of each training batch\n",
    ")\n",
    "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d77f05",
   "metadata": {},
   "source": [
    "##### 2. Model unet 256x256, grid density 1 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca87032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=256,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"unet_256_1m\")     # where to save the model\n",
    ")\n",
    "\n",
    "folder = Path(data_dir)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=1,                       # number of epochs to train\n",
    "    steps_per_epoch=1,             # number of steps per epoch\n",
    "    batch_size=8                   # size of each training batch\n",
    ")\n",
    "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c52511",
   "metadata": {},
   "source": [
    "##### **Jakub** 3. Model unet 512x512, grid density 1 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=512,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=128,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"unet_512_1m\")     # where to save the model\n",
    ")\n",
    "\n",
    "folder = Path(data_dir)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92816269",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=1,                       # number of epochs to train\n",
    "    steps_per_epoch=1,             # number of steps per epoch\n",
    "    batch_size=8                   # size of each training batch\n",
    ")\n",
    "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd455342",
   "metadata": {},
   "source": [
    "#### Grid density 2 m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e0c91",
   "metadata": {},
   "source": [
    "If you have limited disk space, you can adjust `random_cities_count` variable, to download less city grids. It will save time and disk space, but will probably affect model training.\n",
    "If you do this, please write somewhere down, which cities where used to train model - it will help when writing report.\n",
    "\n",
    "> **Note:** It lasts for a while - for Częstochowa it is up to 20 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97215bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scraper.data_loader import DataLoader\n",
    "from trainer.model import Model\n",
    "from trainer.trainer import Trainer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "random_cities_count = 11\n",
    "\n",
    "folder_path = Path(data_dir)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prefered cities to download\n",
    "cities = [\n",
    "    (\"Gdańsk, Polska\", 466630),\n",
    "    (\"Bydgoszcz, Polska\", 358928),\n",
    "    (\"Lublin, Polska\", 339433),\n",
    "    (\"Gdynia, Polska\", 246635),\n",
    "    (\"Radom, Polska\", 214000),\n",
    "    (\"Toruń, Polska\", 202074),\n",
    "    (\"Świętochłowice, Polska\", 55_000),\n",
    "    (\"Koszalin, Polska\", 108_000),\n",
    "    (\"Mielec, Polska\", 59_000),\n",
    "    (\"Chorzów, Polska\", 110_000),\n",
    "    (\"Rybnik, Polska\", 140_000)\n",
    "]\n",
    "\n",
    "# List of Polish cities with population between 50,000 and 500,000\n",
    "# cities = [\n",
    "#     # (\"Warsaw, Polska\", 1790658),\n",
    "#     # (\"Kraków, Polska\", 780981),\n",
    "#     # (\"Łódź, Polska\", 687702),\n",
    "#     # (\"Wrocław, Polska\", 640648),\n",
    "#     # (\"Poznań, Polska\", 538633),\n",
    "#     # (\"Gdańsk, Polska\", 466630),\n",
    "#     (\"Szczecin, Polska\", 402465),\n",
    "#     # (\"Bydgoszcz, Polska\", 358928),\n",
    "#     (\"Lublin, Polska\", 339433),\n",
    "#     (\"Białystok, Polska\", 297459),\n",
    "#     (\"Katowice, Polska\", 294510),\n",
    "#     # (\"Gdynia, Polska\", 246635),\n",
    "#     (\"Częstochowa, Polska\", 224000),\n",
    "#     (\"Radom, Polska\", 214000),\n",
    "#     (\"Toruń, Polska\", 202074),\n",
    "#     (\"Kielce, Polska\", 200000),\n",
    "#     (\"Rzeszów, Polska\", 196000),\n",
    "#     (\"Opole, Polska\", 128000),\n",
    "#     (\"Gliwice, Polska\", 180000),\n",
    "#     (\"Zabrze, Polska\", 170000),\n",
    "#     (\"Elbląg, Polska\", 120000),\n",
    "#     (\"Płock, Polska\", 120000),\n",
    "#     (\"Nowy Sącz, Polska\", 85_000),\n",
    "#     (\"Słupsk, Polska\", 91_000),\n",
    "#     # (\"Świętochłowice, Polska\", 55_000),\n",
    "#     (\"Jelenia Góra, Polska\", 79_000),\n",
    "#     (\"Stalowa Wola, Polska\", 75_000),\n",
    "#     # (\"Koszalin, Polska\", 108_000),\n",
    "#     (\"Mielec, Polska\", 59_000),\n",
    "#     (\"Legnica, Polska\", 100_000),\n",
    "#     (\"Tychy, Polska\", 130_000),\n",
    "#     (\"Chorzów, Polska\", 110_000),\n",
    "#     # (\"Rybnik, Polska\", 140_000)\n",
    "# ]\n",
    "\n",
    "# Randomly select 8 cities\n",
    "random_cities = random.sample(cities, random_cities_count)\n",
    "\n",
    "loader = DataLoader(2, 5000, 5000, data_dir=data_dir)\n",
    "managers = []\n",
    "files = []\n",
    "\n",
    "for city, population in random_cities:\n",
    "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
    "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
    "    manager = loader.load_city_grid(city,file_name)\n",
    "    files.append(file_name)\n",
    "    managers.append(manager)\n",
    "    loader.add_elevation_to_grid(manager)\n",
    "    loader.add_residential_to_grid(manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805aec81",
   "metadata": {},
   "source": [
    "##### 4. Model unet 256x256, grid density 2 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=256,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"unet_256_2m\")     # where to save the model\n",
    ")\n",
    "\n",
    "folder = Path(data_dir)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ee162",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=1,                       # number of epochs to train\n",
    "    steps_per_epoch=1,             # number of steps per epoch\n",
    "    batch_size=8                   # size of each training batch\n",
    ")\n",
    "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa874b",
   "metadata": {},
   "source": [
    "##### 5. Model unet 512x512, grid density 2 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=512,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=128,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"unet_512_2m\")     # where to save the model\n",
    ")\n",
    "\n",
    "folder = Path(data_dir)\n",
    "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "print(f\"files: {files}\")\n",
    "\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=1,                       # number of epochs to train\n",
    "    steps_per_epoch=1,             # number of steps per epoch\n",
    "    batch_size=8                   # size of each training batch\n",
    ")\n",
    "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b8a2d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda74ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "from grid_manager import GridManager\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=256,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\"),     # where to save the model\n",
    "    input_third_dimension=4,\n",
    "    output_third_dimension=3\n",
    ")\n",
    "\n",
    "# folder = Path(data_dir)\n",
    "# files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
    "# print(f\"files: {files}\")\n",
    "\n",
    "# trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "#     model=model,                # the model defined above\n",
    "#     files=files\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6690ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from trainer.model import PREDICT_GRID_INDICES\n",
    "\n",
    "\n",
    "grid_manager = GridManager(\"Bydgoszcz-Polska.city_grid\", data_dir=\"grids/with-is-residential\")\n",
    "metadata = grid_manager.get_metadata()\n",
    "\n",
    "assert metadata.third_dimension_size == 3, \"You have an outdated version of GridManager. Consider downloading grid files again to get all data...\"\n",
    "\n",
    "fragment_row, fragment_col = 2500, 2500\n",
    "fragment_height, fragment_width = 1500, 1500\n",
    "\n",
    "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
    "being_predicted = GridManager(\n",
    "    f\"test_{time.time()}.city_grid\",\n",
    "    fragment_height, fragment_width,\n",
    "    0,0,\n",
    "    metadata.grid_density,\n",
    "    segment_h, segment_w,\n",
    "    data_dir=\"grids/evaluation\",\n",
    "    third_dimension_size=model.input_third_dimension\n",
    ")\n",
    "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
    "# Add IS_PREDICTED\n",
    "tmp = np.ones((fragment_height, fragment_width, model.input_third_dimension))\n",
    "tmp[\n",
    "    :, :,\n",
    "    1:model.input_third_dimension\n",
    "] = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)[\n",
    "    :, :,\n",
    "    :model.input_third_dimension - 1\n",
    "]\n",
    "\n",
    "being_predicted.write_arbitrary_fragment(\n",
    "    tmp,\n",
    "    0, 0\n",
    ") # for instance some segment in the middle\n",
    "\n",
    "result = model.predict(being_predicted)\n",
    "img = result.read_arbitrary_fragment(\n",
    "    0, 0,\n",
    "    fragment_height - model.get_input_grid_surplus(),\n",
    "    fragment_width - model.get_input_grid_surplus()\n",
    ")[:, :, PREDICT_GRID_INDICES.IS_STREET]\n",
    "\n",
    "print(f\"DEBUG: img.max(): {img.max()}\")\n",
    "\n",
    "struct_el = np.ones((3,3))\n",
    "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
    "plt.gray()\n",
    "plt.imshow(dilated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_row, fragment_col = 2000, 2000\n",
    "fragment_height, fragment_width = 3000, 3000\n",
    "\n",
    "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
    "\n",
    "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
    "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
    "print(f\"displayed: {displayed.shape}\")\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(displayed[:, :, 0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d95c4",
   "metadata": {},
   "source": [
    "Version used by Jakub in \"only is_street\" dimension in input data model training tests - below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acaabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from file: models/shallowed_unet_256_1m_is_street_only/1770402047_model.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "from pathlib import Path\n",
    "from grid_manager import GridManager\n",
    "\n",
    "model = ClippingModel(\n",
    "    ClipModels.SHALLOWED_UNET,\n",
    "    clipping_size=256,\n",
    "    clipping_surplus=64,\n",
    "    path=os.path.join(\"models\", \"shallowed_unet_256_1m_is_street_only\"),\n",
    "    input_third_dimension=2,\n",
    "    output_third_dimension=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82be067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: input.third_dimension_size: 2\n",
      "\n",
      "row: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 19:24:46.232863: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........\n",
      "row: 1........\n",
      "row: 2........\n",
      "row: 3........\n",
      "row: 4........\n",
      "row: 5........\n",
      "row: 6........\n",
      "row: 7........DEBUG: img.max(): 0.004070281982421875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56cd39dca0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_manager import GridManager\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from trainer.model import PREDICT_GRID_INDICES\n",
    "\n",
    "\n",
    "grid_manager = GridManager(\"Bydgoszcz-Polska.city_grid\", data_dir=\"grids/with-is-residential\")\n",
    "metadata = grid_manager.get_metadata()\n",
    "\n",
    "assert metadata.third_dimension_size == 3, \"You have an outdated version of GridManager. Consider downloading grid files again to get all data...\"\n",
    "\n",
    "fragment_row, fragment_col = 2500, 2500\n",
    "fragment_height, fragment_width = 1500, 1500\n",
    "\n",
    "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
    "being_predicted = GridManager(\n",
    "    f\"test_{time.time()}.city_grid\",\n",
    "    fragment_height, fragment_width,\n",
    "    0,0,\n",
    "    metadata.grid_density,\n",
    "    segment_h, segment_w,\n",
    "    data_dir=\"grids/evaluation\",\n",
    "    third_dimension_size=model.input_third_dimension\n",
    ")\n",
    "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
    "\n",
    "crop = model.get_input_grid_surplus() // 2  # zwykle input_surplus/2\n",
    "\n",
    "# Add IS_PREDICTED\n",
    "tmp = np.zeros((fragment_height, fragment_width, model.input_third_dimension))\n",
    "\n",
    "tmp[\n",
    "    :, :, 1\n",
    "] = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)[\n",
    "    :, :, 0\n",
    "].astype(np.float32)\n",
    "\n",
    "# 2) ustaw maskę: przewidujemy tylko środek\n",
    "tmp[:, :, 0] = 0.0\n",
    "tmp[crop:fragment_height-crop, crop:fragment_width-crop, 0] = 1.0\n",
    "\n",
    "# (opcjonalnie) możesz ręcznie wyzerować środek, ale jeśli predict() woła clean_input(), to nie jest konieczne:\n",
    "# tmp[crop:fragment_height-crop, crop:fragment_width-crop, 1] = 0.0\n",
    "\n",
    "being_predicted.write_arbitrary_fragment(\n",
    "    tmp,\n",
    "    0, 0\n",
    ") # for instance some segment in the middle\n",
    "\n",
    "result = model.predict(being_predicted)\n",
    "img = result.read_arbitrary_fragment(\n",
    "    0, 0,\n",
    "    fragment_height - model.get_input_grid_surplus(),\n",
    "    fragment_width - model.get_input_grid_surplus()\n",
    ")[:, :, PREDICT_GRID_INDICES.IS_STREET]\n",
    "\n",
    "print(f\"DEBUG: img.max(): {img.max()}\")\n",
    "\n",
    "struct_el = np.ones((3,3))\n",
    "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
    "plt.gray()\n",
    "plt.imshow(dilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafe4d0",
   "metadata": {},
   "source": [
    "old version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from trainer.model import PREDICT_GRID_INDICES\n",
    "\n",
    "\n",
    "grid_manager = GridManager(\"Bydgoszcz-Polska.city_grid\", data_dir=\"grids/with-is-residential\")\n",
    "metadata = grid_manager.get_metadata()\n",
    "\n",
    "assert metadata.third_dimension_size == 3, \"You have an outdated version of GridManager. Consider downloading grid files again to get all data...\"\n",
    "\n",
    "fragment_row, fragment_col = 2500, 2500\n",
    "fragment_height, fragment_width = 1500, 1500\n",
    "\n",
    "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
    "being_predicted = GridManager(\n",
    "    f\"test_{time.time()}.city_grid\",\n",
    "    fragment_height, fragment_width,\n",
    "    0,0,\n",
    "    metadata.grid_density,\n",
    "    segment_h, segment_w,\n",
    "    data_dir=\"grids/evaluation\",\n",
    "    third_dimension_size=2\n",
    ")\n",
    "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
    "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
    "print(f\"displayed: {displayed.shape}\")\n",
    "\n",
    "being_predicted.write_arbitrary_fragment(\n",
    "    grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)[:, :, :2],\n",
    "    0, 0\n",
    ") # for instance some segment in the middle\n",
    "\n",
    "result = model.predict(being_predicted)\n",
    "img = result.read_arbitrary_fragment(\n",
    "    0, 0,\n",
    "    fragment_height - model.get_input_grid_surplus(),\n",
    "    fragment_width - model.get_input_grid_surplus()\n",
    ")[:, :, PREDICT_GRID_INDICES.IS_STREET]\n",
    "\n",
    "print(f\"DEBUG: img.max(): {img.max()}\")\n",
    "\n",
    "struct_el = np.ones((3,3))\n",
    "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
    "plt.gray()\n",
    "plt.imshow(dilated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d12be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56ff71dca0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af837657",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_row, fragment_col = 2000, 2000\n",
    "fragment_height, fragment_width = 3000, 3000\n",
    "\n",
    "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
    "\n",
    "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
    "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
    "print(f\"displayed: {displayed.shape}\")\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(displayed[:, :, 0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f8fae",
   "metadata": {},
   "source": [
    "## Examples of modules usages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402120c",
   "metadata": {},
   "source": [
    "### Download mesh and get grid from OSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0390497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.data_loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec41555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!! - dataloader checks, if the file already exists\n",
    "from scraper.data_loader import DataLoader\n",
    "\n",
    "cityname = \"Kraków\"\n",
    "\n",
    "loader = DataLoader(1)\n",
    "\n",
    "grid_manager = loader.load_city_grid(cityname, cityname + \".dat\")\n",
    "# loader.add_elevation_to_grid(grid_manager)\n",
    "loader.add_residential_to_grid(grid_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399ff60",
   "metadata": {},
   "source": [
    "#### Display downloaded grid (dilated to be better visible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "grid_manager = GridManager(\"Tychy.dat\", data_dir=\"grids\")\n",
    "img = grid_manager.read_segment(2,0)[:5000, :5000, 0]\n",
    "\n",
    "struct_el = np.ones((3,3))\n",
    "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
    "plt.gray()\n",
    "plt.imshow(dilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853958a1",
   "metadata": {},
   "source": [
    "#### Adding is residential tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!! - dataloader checks, if the file already exists\n",
    "from scraper.data_loader import DataLoader\n",
    "\n",
    "cityname = \"Tychy\"\n",
    "\n",
    "loader = DataLoader(1)\n",
    "\n",
    "grid_manager = loader.load_city_grid(cityname, cityname + \".dat\")\n",
    "# loader.add_elevation_to_grid(grid_manager)\n",
    "loader.add_residential_to_grid(grid_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "from scraper.rasterizer import Rasterizer \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "grid_manager = GridManager(\"Toruń.dat\")\n",
    "meta = grid_manager.get_metadata()\n",
    "segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
    "segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
    "\n",
    "\n",
    "rasterizer = Rasterizer()\n",
    "fig, axes = plt.subplots(segments_rows, segments_cols)\n",
    "fig.set_size_inches((16, 16))\n",
    "axes = axes.flatten()\n",
    "axes_index = 0\n",
    "\n",
    "for row_idx in range(segments_rows):\n",
    "    for col_idx in range(segments_cols):\n",
    "        segment = grid_manager.read_segment(row_idx, col_idx)\n",
    "\n",
    "        axes[axes_index].imshow(segment[:, :, 2], cmap=\"gray\")\n",
    "        axes[axes_index].set_title(f\"Segment_rows: {row_idx}, segment_cols: {col_idx}\")\n",
    "        axes_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69284905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "from scraper.rasterizer import Rasterizer \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "grid_manager = GridManager(\"Toruń.dat\")\n",
    "meta = grid_manager.get_metadata()\n",
    "segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
    "segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
    "\n",
    "\n",
    "rasterizer = Rasterizer()\n",
    "fig, axes = plt.subplots(segments_rows, segments_cols)\n",
    "fig.set_size_inches((16, 16))\n",
    "axes = axes.flatten()\n",
    "axes_index = 0\n",
    "\n",
    "for row_idx in range(segments_rows):\n",
    "    for col_idx in range(segments_cols):\n",
    "        segment = grid_manager.read_segment(row_idx, col_idx)\n",
    "\n",
    "        axes[axes_index].imshow(segment[:, :, 0], cmap=\"gray\")\n",
    "        axes[axes_index].set_title(f\"Segment_rows: {row_idx}, segment_cols: {col_idx}\")\n",
    "        axes_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "\n",
    "grid_manager = GridManager(\"Tychy.dat\")\n",
    "img = grid_manager.read_segment(1,1)[:2000, :2000, 0]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,5))\n",
    "fig.suptitle(\"Obrazki dróg (zdylatowane, żeby było lepiej widać)\")\n",
    "\n",
    "imgs = [\n",
    "    [grid_manager.read_segment(0,0)[3000:, 3000:, 0], grid_manager.read_segment(0,1)[3000:, :2000, 0]],\n",
    "    [grid_manager.read_segment(1,0)[:2000, 3000:, 0], grid_manager.read_segment(1,1)[:2000, :2000, 0]]\n",
    "]\n",
    "\n",
    "dilated_imgs = []\n",
    "\n",
    "struct_el = np.ones((3,3))\n",
    "\n",
    "for imgs_row in imgs:\n",
    "    target_row = []\n",
    "    dilated_imgs.append(target_row)\n",
    "    for img in imgs_row:\n",
    "        target_row.append(cv2.dilate(img, struct_el, iterations=3))\n",
    "\n",
    "plt.gray()\n",
    "axs[0,0].imshow(dilated_imgs[0][0])\n",
    "axs[0,1].imshow(dilated_imgs[0][1])\n",
    "axs[1,0].imshow(dilated_imgs[1][0])\n",
    "axs[1,1].imshow(dilated_imgs[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863446e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.grid_builder import GridBuilder\n",
    "import math\n",
    "from scraper.rasterizer import Rasterizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "city = \"Tychy, Polska\"\n",
    "builder = GridBuilder()\n",
    "\n",
    "gdf_edges = builder.get_city_roads(city)\n",
    "\n",
    "rasterizer = Rasterizer()\n",
    "\n",
    "grid_2d = rasterizer.get_rasterize_roads(gdf_edges, 1, is_residential=False)\n",
    "plt.imshow(grid_2d, cmap=\"gray\")\n",
    "plt.title(f\"City: {city}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af730340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.grid_builder import GridBuilder\n",
    "import math\n",
    "from scraper.rasterizer import Rasterizer\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "city = \"Kraków\"\n",
    "builder = GridBuilder()\n",
    "grid_density = 1\n",
    "segment_h = 5000\n",
    "segment_w = 5000\n",
    "data_dir = \"grids\"\n",
    "\n",
    "gdf_edges = builder.get_city_roads(city)\n",
    "min_x, min_y, max_x, max_y = gdf_edges.total_bounds\n",
    "\n",
    "columns_number = math.ceil((max_x - min_x) / grid_density)\n",
    "rows_number = math.ceil((max_y - min_y) / grid_density)\n",
    "\n",
    "segment_rows = math.ceil((rows_number) / segment_h)\n",
    "segment_cols = math.ceil((columns_number) / segment_h)\n",
    "\n",
    "# grid_manager = GridManager(file_name, rows_number=int(rows_number), columns_number=int(columns_number),\n",
    "#                             grid_density=grid_density, segment_h=segment_h, segment_w=segment_w,\n",
    "#                             data_dir=data_dir, upper_left_longitude=min_x, upper_left_latitude=max_y)\n",
    "print(f\"Height: {int(rows_number)}, Width: {int(columns_number)}, rows: {segment_rows}, cols: {segment_cols}\")\n",
    "\n",
    "rasterizer = Rasterizer()\n",
    "fig, axes = plt.subplots(segment_rows, segment_cols)\n",
    "fig.set_size_inches((16, 16))\n",
    "axes = axes.flatten()\n",
    "axes_index = 0\n",
    "# single_row_gdf = gdf_edges.sample(1)\n",
    "\n",
    "# grid_2d = rasterizer.rasterize_segment_from_indexes(gdf_edges=single_row_gdf, indexes=(0, 0), is_residential=False\n",
    "#                                                             size_h=segment_h, size_w=segment_w,\n",
    "#                                                             pixel_size=grid_density)\n",
    "# print(grid_2d.shape)\n",
    "# plt.imshow(grid_2d, cmap=\"gray\")\n",
    "\n",
    "for i in range(segment_rows):\n",
    "    for j in range(segment_cols):\n",
    "        grid_2d = rasterizer.rasterize_segment_from_indexes(gdf_edges=gdf_edges, indexes=(i, j), is_residential=False,\n",
    "                                                            size_h=segment_h, size_w=segment_w,\n",
    "                                                            pixel_size=grid_density)\n",
    "        axes[axes_index].imshow(grid_2d, cmap=\"gray\")\n",
    "        axes[axes_index].set_title(f\"Segment_rows: {i}, segment_cols: {j}\")\n",
    "        axes_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8acb1",
   "metadata": {},
   "source": [
    "### Data management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_manager import GridManager\n",
    "import numpy as np\n",
    "\n",
    "filename = \"przyklad1.dat\"\n",
    "\n",
    "src_man = GridManager(filename, 2000, 2000, 0.0, 0.0, 1, 3, 3, data_dir=\"grids\")\n",
    "\n",
    "# a = np.zeros((2000, 2000, 2), dtype=np.float64)\n",
    "is_street = np.array([\n",
    "    [1, 2, 3, 4, 5, 6],\n",
    "    [11, 12, 13, 14, 15, 16],\n",
    "    [21, 22, 23, 24, 25, 26],\n",
    "    [31, 32, 33, 34, 35, 36],\n",
    "    [41, 42, 43, 44, 45, 46],\n",
    "    [51, 52, 53, 54, 55, 56],\n",
    "])\n",
    "a = np.zeros((is_street.shape[0], is_street.shape[1], 3))\n",
    "a[:,:, 0] = is_street\n",
    "\n",
    "for x in range(2):\n",
    "    for y in range(2):\n",
    "        src_man.write_segment(a[y * 3: (y + 1) * 3, x*3:(x + 1) * 3], y, x)\n",
    "\n",
    "\n",
    "src_man.write_segment(a[:2,:3, :] + 1, 666, 0)\n",
    "src_man.write_segment(a[:3,:2, :] + 2, 0, 666)\n",
    "\n",
    "man = src_man.deep_copy()\n",
    "\n",
    "print(f\"1: {man.read_segment(0, 0)[:, :, 0]}\")\n",
    "print(f\"2: {man.read_segment(0, 1)[:, :, 0]}\")\n",
    "print(f\"3: {man.read_segment(1, 0)[:, :, 0]}\")\n",
    "print(f\"4: {man.read_segment(1, 1)[:, :, 0]}\")\n",
    "print(f\"5: {man.read_segment(666, 666)[:, :, 0]}\")\n",
    "\n",
    "print(man.get_metadata())\n",
    "\n",
    "man.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a46640",
   "metadata": {},
   "source": [
    "### Streets discovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0770ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_crossroads(img_height, img_width, conflictless_crossroads, conflicting_crossroads):\n",
    "    fig, axs = plt.subplots(  max(2, len(conflictless_crossroads), len(conflicting_crossroads)), 6, figsize=(10,5))\n",
    "    axs[0, 0].set_title(\"conflictless\")\n",
    "    axs[0, 1].set_title(\"junctions\")\n",
    "    axs[0, 2].set_title(\"conflicts\")\n",
    "    axs[0, 3].set_title(\"conflicting\")\n",
    "    axs[0, 4].set_title(\"junctions\")\n",
    "    axs[0, 5].set_title(\"conflicts\")\n",
    "\n",
    "\n",
    "    for i in range(len(conflictless_crossroads)):\n",
    "        crossroad = conflictless_crossroads[i]\n",
    "\n",
    "        crossroad_image = np.zeros((img_height, img_width))\n",
    "        for point in crossroad.points:\n",
    "            crossroad_image[point] = 1\n",
    "\n",
    "        axs[i, 0].imshow(crossroad_image)\n",
    "\n",
    "    for i in range(len(conflictless_crossroads)):\n",
    "        crossroad = conflictless_crossroads[i]\n",
    "\n",
    "        crossroad_image = np.zeros((img_height, img_width))\n",
    "        for street in crossroad.street_junctions.keys():\n",
    "            crossroad_image[crossroad.street_junctions[street]] = 1\n",
    "\n",
    "        axs[i, 1].imshow(crossroad_image)\n",
    "\n",
    "    for i in range(len(conflictless_crossroads)):\n",
    "        crossroad = conflictless_crossroads[i]\n",
    "\n",
    "        crossroad_image = np.zeros((img_height, img_width))\n",
    "        for point in crossroad.conflicting_points:\n",
    "            crossroad_image[point] = 1\n",
    "\n",
    "        axs[i, 2].imshow(crossroad_image)\n",
    "\n",
    "    for i in range(len(conflicting_crossroads)):\n",
    "        crossroad = conflicting_crossroads[i]\n",
    "\n",
    "        crossroad_image = np.zeros((img_height, img_width))\n",
    "        for point in crossroad.points:\n",
    "            crossroad_image[point] = 1\n",
    "\n",
    "        axs[i, 3].imshow(crossroad_image)\n",
    "\n",
    "    for i in range(len(conflicting_crossroads)):\n",
    "        crossroad = conflicting_crossroads[i]\n",
    "\n",
    "        crossroad_image = np.zeros((img_height, img_width))\n",
    "        for street in crossroad.street_junctions.keys():\n",
    "            crossroad_image[crossroad.street_junctions[street]] = 1\n",
    "\n",
    "        axs[i, 4].imshow(crossroad_image)\n",
    "\n",
    "    for i in range(len(conflicting_crossroads)):\n",
    "        crossroad = conflicting_crossroads[i]\n",
    "\n",
    "        crossroad_image = np.zeros((img_height, img_width))\n",
    "        for point in crossroad.conflicting_points:\n",
    "            crossroad_image[point] = 1\n",
    "\n",
    "        axs[i, 5].imshow(crossroad_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_streets(img_height, img_width, conflictless_streets, conflicting_streets):\n",
    "    fig, axs = plt.subplots(  max(2, len(conflictless_streets), len(conflicting_streets)), 4, figsize=(10,8))\n",
    "    axs[0, 0].set_title(\"conflictless\")\n",
    "    axs[0, 1].set_title(\"conflicts\")\n",
    "    axs[0, 2].set_title(\"conflicting\")\n",
    "    axs[0, 3].set_title(\"conflicts\")\n",
    "\n",
    "\n",
    "    for i in range(len(conflictless_streets)):\n",
    "        street = conflictless_streets[i]\n",
    "\n",
    "        street_image = np.zeros((img_height, img_width))\n",
    "        for point in street.linestring:\n",
    "            street_image[point] = 1\n",
    "\n",
    "        axs[i, 0].imshow(street_image)\n",
    "    \n",
    "    for i in range(len(conflictless_streets)):\n",
    "        street = conflictless_streets[i]\n",
    "\n",
    "        street_image = np.zeros((img_height, img_width))\n",
    "        for point in street.conflicts:\n",
    "            street_image[point] = 1\n",
    "\n",
    "        axs[i, 1].imshow(street_image)\n",
    "\n",
    "    for i in range(len(conflicting_streets)):\n",
    "        street = conflicting_streets[i]\n",
    "\n",
    "        street_image = np.zeros((img_height, img_width))\n",
    "        for point in street.linestring:\n",
    "            street_image[point] = 1\n",
    "\n",
    "        axs[i, 2].imshow(street_image)\n",
    "    \n",
    "    for i in range(len(conflicting_streets)):\n",
    "        street = conflicting_streets[i]\n",
    "\n",
    "        street_image = np.zeros((img_height, img_width))\n",
    "        for point in street.conflicts:\n",
    "            street_image[point] = 1\n",
    "\n",
    "        axs[i, 3].imshow(street_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f547aa",
   "metadata": {},
   "source": [
    "#### Diamond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.morphology import skeletonize\n",
    "from graph_remaker.morphological_remaker import discover_streets\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"test_images/diamond.png\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.gray()\n",
    "bin_image = image > 0\n",
    "# plt.imshow(bin_image * 255)\n",
    "\n",
    "processed_image = bin_image[:, :, np.newaxis]\n",
    "processed_image = np.concatenate((processed_image, np.zeros(processed_image.shape)), axis=2)\n",
    "\n",
    "conflictless_crossroads, conflicting_crossroads, conflictless_streets, conflicting_streets = discover_streets(processed_image)\n",
    "\n",
    "height, width = image.shape\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_crossroads(height, width, conflictless_crossroads, conflicting_crossroads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a21635",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_streets(height, width, conflictless_streets, conflicting_streets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0960ab",
   "metadata": {},
   "source": [
    "#### Conflicting crossroad and dead-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.morphology import skeletonize\n",
    "from graph_remaker.morphological_remaker import discover_streets\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"test_images/crossroad_conflict_deadend.png\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.gray()\n",
    "bin_image = image > 0\n",
    "# plt.imshow(bin_image * 255)\n",
    "\n",
    "processed_image = bin_image[:, :, np.newaxis]\n",
    "processed_image = np.concatenate((processed_image, np.zeros(processed_image.shape)), axis=2)\n",
    "\n",
    "conflictless_crossroads, conflicting_crossroads, conflictless_streets, conflicting_streets = discover_streets(processed_image)\n",
    "\n",
    "height, width = image.shape\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ed360",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_crossroads(width, height, conflictless_crossroads, conflicting_crossroads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_streets(width, height, conflictless_streets, conflicting_streets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e175b48",
   "metadata": {},
   "source": [
    "### Model training (tutorial, not used in practice anymore. For practical use, look at the higher entries in the notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c66cd",
   "metadata": {},
   "source": [
    "##### Use the whole VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_model import tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=4000)] # Limit to 4000 MB of VRAM - TODO: adjust based on your GPU\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2a0e3",
   "metadata": {},
   "source": [
    "##### Create batch sequence of clippings that are the candidates for a model and print the shape of the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51faed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_sequence import ClippingBatchSequence # not used anymore in the project - only for visualisation\n",
    "from trainer.batch_sequence import BatchSequence            # not used anymore in the project - only for visualisation\n",
    "\n",
    "batchSeq = ClippingBatchSequence(           # Define a clipping batch sequence\n",
    "            BatchSequence(                      # Define a batch sequence\n",
    "                files=list([\"Tychy.dat\"]),          # list of files to use\n",
    "                batch_size=1,                       # not important here\n",
    "                cut_sizes=[(512, 512)],             # sizes of cuttings to make from the grid\n",
    "            ),\n",
    "            clipping_size=512,                  # size of the clipping (input to the model)\n",
    "            input_grid_surplus=64,              # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "        )\n",
    "\n",
    "X, Y = batchSeq[0]          # get the first batch\n",
    "print(\"Shape of the input batch: \", X.shape)              # print the shape of the input batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9534c",
   "metadata": {},
   "source": [
    "##### Display example clipping (X and y) - `WARNING: you may need to run the above code up to approx. 3-5 times in order to get an image below, other than a void one`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ae7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "axs[0].imshow(X[0, :, :, 0], cmap='grey')\n",
    "axs[1].imshow(Y[\"is_street\"][0, :, :, 0], cmap='grey')\n",
    "\n",
    "print(\"X min/max:\", X[0, :, :, 0].min(), X[0, :, :, 0].max())\n",
    "print(\"Y min/max:\", Y[\"is_street\"][0, :, :, 0].min(), Y[\"is_street\"][0, :, :, 0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a9a19",
   "metadata": {},
   "source": [
    "##### Show available GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aabc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11892b0d",
   "metadata": {},
   "source": [
    "##### Define a model and then cities for the trainer (e.g. Tychy) as the files list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_model import ClipModels       # Import ClipModels Enum\n",
    "\n",
    "print(\"Possible models:\")\n",
    "for model in ClipModels:\n",
    "    print(f\"- {model.name}: {model.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "\n",
    "model = ClippingModel(      # Define the model\n",
    "    ClipModels.BASE,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
    "    clipping_size=512,          # size of the clipping (input to the model)\n",
    "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
    "    path=\"model_test_16_01\"     # where to save the model\n",
    ")\n",
    "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
    "    model=model,                # the model defined above\n",
    "    files=[                     # list of data files to use for training\n",
    "        \"Tychy.dat\",\n",
    "        # \"Kraków.dat\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5b859",
   "metadata": {},
   "source": [
    "##### Run fitting which created the above batch sequence and trains the model\n",
    "\n",
    "`random_fit_from_files(self, epochs: int = 100, steps_per_epoch=1000, batch_size=32)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.random_fit_from_files(  # Train the model with random data from files\n",
    "    epochs=1,                       # number of epochs to train\n",
    "    steps_per_epoch=10,             # number of steps per epoch\n",
    "    batch_size=32                   # size of each training batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c07e9",
   "metadata": {},
   "source": [
    "##### Other models etc.\n",
    "\n",
    "You may need to run \"Use the whole VRAM\" cell before them (separately and probably after the kernel restart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880c096",
   "metadata": {},
   "source": [
    "#### UNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "model = ClippingModel(\n",
    "    ClipModels.UNET,\n",
    "    clipping_size=512,\n",
    "    clipping_surplus=64,\n",
    "    path=\"model_test_13_01_13_22\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    files=[\n",
    "        \"Tychy.dat\",\n",
    "        \"Kraków.dat\"\n",
    "    ]\n",
    ")\n",
    "# Na mojej laptopowej RTX 3050 Ti 4GB, batch_size=8 nie poradził sobie z pamięcią po 4 minutach.\n",
    "# Zmieniłem batch_size na 2 i działa - 44 sekundy na poniższym przykładzie\n",
    "# UPDATE: Zmieniłem na 4, działa dobrze\n",
    "# - trochę ponad minuta po ustawieniu (w wyższej komórce) memory_limit=4000\n",
    "trainer.random_fit_from_files(1, 10, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f82b17",
   "metadata": {},
   "source": [
    "#### AlexInspired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0da5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
    "from trainer.trainer import Trainer\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "model = ClippingModel(\n",
    "    ClipModels.ALEX_INSPIRED,\n",
    "    clipping_size=256,\n",
    "    clipping_surplus=32,\n",
    "    path=\"model_test_13_01_13_22\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    files=[\n",
    "        \"Tychy.dat\",\n",
    "        \"Kraków.dat\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# u mnie zajęło 41 minut i 1.1 sekundy\n",
    "trainer.random_fit_from_files(1, 10, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864c4df36b3e3e0",
   "metadata": {},
   "source": [
    "### upper algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceac6721d76834b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:38:49.502971Z",
     "start_time": "2026-02-06T16:38:25.433763Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Upewnij się, że masz te importy dostępne w projekcie\n",
    "from grid_manager import GridManager, GRID_INDICES\n",
    "from graph_remaker.memory_wise import process_large_grid\n",
    "\n",
    "# 1. KONFIGURACJA\n",
    "H, W = 1000, 1000\n",
    "SEG_SIZE = 200\n",
    "TEMP_GRID_DIR = \"grids\"\n",
    "TEMP_GRID_FILE = \"square_test.dat\"\n",
    "FULL_PATH = os.path.join(TEMP_GRID_DIR, TEMP_GRID_FILE)\n",
    "\n",
    "# Upewnij się, że katalog istnieje\n",
    "os.makedirs(TEMP_GRID_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Generowanie obrazu {W}x{H}...\")\n",
    "\n",
    "# Tworzymy macierz 3-kanałową (zgodnie z oczekiwaniami memory_wise/morphological_remaker)\n",
    "# Zazwyczaj: 0=IS_STREET, 1=ALTITUDE, 2=IS_ESTATE (lub podobnie)\n",
    "full_grid = np.zeros((H, W, 3), dtype=np.float32)\n",
    "\n",
    "# Tymczasowe płótno do rysowania kształtów (OpenCV operuje na uint8)\n",
    "canvas = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "# Rysujemy KWADRAT (linie prostopadłe do osi)\n",
    "# Współrzędne: (150, 150) do (850, 850)\n",
    "cv2.rectangle(canvas, (150, 150), (850, 850), 255, thickness=12)\n",
    "\n",
    "# Dodajmy Krzyż w środku (skrzyżowanie 4-wylotowe)\n",
    "cv2.line(canvas, (500, 150), (500, 850), 255, thickness=12) # Pion\n",
    "cv2.line(canvas, (150, 500), (850, 500), 255, thickness=12) # Poziom\n",
    "\n",
    "# Przepisanie do odpowiedniego kanału w Gridzie\n",
    "# IS_STREET - maska binarna (0 lub 1)\n",
    "full_grid[:, :, GRID_INDICES.IS_STREET] = (canvas > 0).astype(np.float32)\n",
    "\n",
    "# ALTITUDE - zostawiamy 0.0 (płaski teren), żeby morphological_remaker nie wyrzucił błędu przy slope\n",
    "full_grid[:, :, GRID_INDICES.ALTITUDE] = 0.0\n",
    "\n",
    "print(\"Obraz wygenerowany.\")\n",
    "\n",
    "# 2. ZAPIS DO PLIKU POPRZEZ GRID MANAGER\n",
    "\n",
    "# Używamy pełnej ścieżki tylko do sprawdzenia i usunięcia starego pliku\n",
    "if os.path.exists(FULL_PATH):\n",
    "    try:\n",
    "        os.remove(FULL_PATH)\n",
    "    except PermissionError:\n",
    "        print(f\"Nie można usunąć {FULL_PATH}, może jest otwarty? Próbuję nadpisać...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd przy usuwaniu pliku: {e}\")\n",
    "\n",
    "# Inicjalizacja Managera\n",
    "# ZMIANA:\n",
    "# 1. file_name = TEMP_GRID_FILE (samo \"square_test.dat\")\n",
    "# 2. data_dir = TEMP_GRID_DIR (\"grids\")\n",
    "# GridManager sam połączy to w \"grids/square_test.dat\"\n",
    "gm = GridManager(file_name=TEMP_GRID_FILE,\n",
    "                 rows_number=H,\n",
    "                 columns_number=W,\n",
    "                 upper_left_longitude=0.0,\n",
    "                 upper_left_latitude=0.0,\n",
    "                 grid_density=1.0,\n",
    "                 segment_h=SEG_SIZE,\n",
    "                 segment_w=SEG_SIZE,\n",
    "                 data_dir=TEMP_GRID_DIR)\n",
    "\n",
    "rows_n = math.ceil(H / SEG_SIZE)\n",
    "cols_n = math.ceil(W / SEG_SIZE)\n",
    "\n",
    "print(f\"Zapisywanie segmentów ({rows_n}x{cols_n})...\")\n",
    "\n",
    "for r in range(rows_n):\n",
    "    for c in range(cols_n):\n",
    "        y_start = r * SEG_SIZE\n",
    "        x_start = c * SEG_SIZE\n",
    "        y_end = min(y_start + SEG_SIZE, H)\n",
    "        x_end = min(x_start + SEG_SIZE, W)\n",
    "\n",
    "        # Wycinamy fragment z pełnej siatki 3D\n",
    "        chunk = full_grid[y_start:y_end, x_start:x_end, :]\n",
    "        gm.write_segment(chunk, r, c)\n",
    "\n",
    "print(\"Dane zapisane na dysku. Uruchamiam memory_wise...\")\n",
    "\n",
    "# 3. URUCHOMIENIE PRZETWARZANIA\n",
    "try:\n",
    "    print(f\"Wczytywanie pliku: {TEMP_GRID_FILE} z domyślnego folderu 'grids'...\")\n",
    "\n",
    "    # ZMIANA: Przekazujemy tylko nazwę pliku (\"square_test.dat\"), a nie FULL_PATH.\n",
    "    # GridManager wewnątrz tej funkcji sam dopisze sobie folder \"grids/\".\n",
    "    graph = process_large_grid(TEMP_GRID_FILE)\n",
    "\n",
    "    # 4. WIZUALIZACJA\n",
    "    print(f\"\\n--- WYNIK ---\")\n",
    "    print(f\"Węzły: {len(graph.nodes)}\")\n",
    "    print(f\"Krawędzie: {len(graph.edges)}\")\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # --- LEWA STRONA: Oryginalny obraz (Piksele) ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Wejście (Piksele {W}x{H})\")\n",
    "    plt.imshow(canvas, cmap='gray')\n",
    "\n",
    "    # Siatka podziału na segmenty\n",
    "    for y in range(0, H, SEG_SIZE):\n",
    "        plt.axhline(y, color='yellow', linestyle='--', alpha=0.3)\n",
    "    for x in range(0, W, SEG_SIZE):\n",
    "        plt.axvline(x, color='yellow', linestyle='--', alpha=0.3)\n",
    "    plt.xlabel(\"X [px]\")\n",
    "    plt.ylabel(\"Y [px]\")\n",
    "\n",
    "    # --- PRAWA STRONA: Zrekonstruowany Graf (Lat/Lon) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Wyjście: Graf (Współrzędne Geograficzne)\")\n",
    "\n",
    "    if len(graph.nodes) > 0:\n",
    "        # Rysowanie węzłów\n",
    "        # Memory_wise konwertuje piksele na Lat/Lon.\n",
    "        # node['x'] to Longitude, node['y'] to Latitude.\n",
    "        node_positions = {n: (data['x'], data['y']) for n, data in graph.nodes(data=True)}\n",
    "\n",
    "        # Rysowanie krawędzi (GeoPandas style - używając geometrii LineString)\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            if 'geometry' in data and isinstance(data['geometry'], LineString):\n",
    "                # Jeśli mamy geometrię szczegółową (krzywe)\n",
    "                xs, ys = data['geometry'].xy\n",
    "                plt.plot(xs, ys, color='cyan', linewidth=2, alpha=0.7, zorder=1)\n",
    "            else:\n",
    "                # Fallback dla prostych linii\n",
    "                p1 = node_positions[u]\n",
    "                p2 = node_positions[v]\n",
    "                plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color='blue', linewidth=1, alpha=0.5, zorder=1)\n",
    "\n",
    "        # Rysowanie punktów węzłów\n",
    "        # Rozróżnienie kolorami: Skrzyżowania vs Łączniki\n",
    "        crossroads = [n for n, d in graph.nodes(data=True) if d.get('type') == 'crossroad']\n",
    "        connectors = [n for n, d in graph.nodes(data=True) if d.get('type') != 'crossroad']\n",
    "\n",
    "        nx.draw_networkx_nodes(graph, node_positions, nodelist=crossroads, node_size=30, node_color='red', label='Crossroads')\n",
    "        nx.draw_networkx_nodes(graph, node_positions, nodelist=connectors, node_size=10, node_color='orange', label='Connectors')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.axis('equal')\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"BRAK WYNIKÓW GRAFU\", ha='center', color='red', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "croada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
