{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "f7133f1d",
         "metadata": {},
         "source": [
            "## Training target model\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "478c05d3",
         "metadata": {},
         "source": [
            "### Introduction"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "abce8ff6",
         "metadata": {},
         "source": [
            "1. Pull the latest version of repository - to see, which models were already chosen by others.\n",
            "2. Install new `requirements.txt`. It should work with python 3.12 and upgraded pip. It contains cuda tensrflow version. If you have problems with installation, try with `requirements-no-gpu.txt`.\n",
            "3. Check, if tensorflow sees your graphics card.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ba232e4c",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Source - https://stackoverflow.com/a\n",
            "# Posted by Wilmar van Ommeren, modified by community. See post 'Timeline' for change history\n",
            "# Retrieved 2026-01-18, License - CC BY-SA 4.0\n",
            "\n",
            "import tensorflow as tf\n",
            "tf.config.list_physical_devices('GPU')"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bf9dafc6",
         "metadata": {},
         "source": [
            "it should return something like:\n",
            "\n",
            "```\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "```\n",
            "\n",
            "4. Running model involves preparing GPU, downloading data and running appropriate training. Everything is described below.\n",
            "5. And don't forget to push your name next to chosen model to repository!\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9c4ce42f",
         "metadata": {},
         "source": [
            "#### Use the whole VRAM\n",
            "\n",
            "Set VRAM size accoring to your GPU!!\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c176fb3f",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import tf\n",
            "\n",
            "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
            "if gpus:\n",
            "    try:\n",
            "        for gpu in gpus:\n",
            "            tf.config.experimental.set_memory_growth(gpu, False)\n",
            "        tf.config.set_logical_device_configuration(\n",
            "            gpus[0],\n",
            "            [tf.config.LogicalDeviceConfiguration(memory_limit=6000)] # Limit to 6000 MB of VRAM - TODO: adjust based on your GPU\n",
            "        )\n",
            "    except RuntimeError as e:\n",
            "        print(e)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9ac5dc5f",
         "metadata": {},
         "source": [
            "### Loading data and training model\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "186010ba",
         "metadata": {},
         "source": [
            "Model training subsections are divided by grid density. Choose one model training subsection and then please clear cell outputs, **ADD your name in chosen model section header** and push this change on the repository. This should help us avoid conflicts, that 2 people will choose the same model.\n",
            "\n",
            "So the header sholud like this:\n",
            "\n",
            "```markdown\n",
            "##### **GRZEGORZ** 1. Model - shallowed unet 256x256, grid density 1 m\n",
            "```\n",
            "\n",
            "#### trainer.random_fit_from_files() arguments\n",
            "\n",
            "At the moment `epoch_steps` and `epochs` are set to 1. Run it and check, how long does it take to execute it. Then adjust these values to the amout of time you have. `epoch_steps` over 200 probably doesn't make sense, so later on increase just `epochs`.\n",
            "\n",
            "If it lasts over 15 minutes, it means that probably there is some problem with graphics card. Nevertheless, it should run properly. Model is dumped to a different the file after each training and starts from the last checkpoint everytime, so don't hesitate to run it multiple times, if you want/have time to do it.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "2f87fb9a",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "\n",
            "data_dir = os.path.join(\"grids\", \"with-is-residential\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c2779acc",
         "metadata": {},
         "source": [
            "#### Grid density 1m\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "56067167",
         "metadata": {},
         "source": [
            "##### Downloading cities\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "051c35b1",
         "metadata": {},
         "source": [
            "If you have limited disk space, you can adjust `random_cities_count` variable, to download less city grids. It will save time and disk space, but will probably affect model training.\n",
            "If you do this, please write somewhere down, which cities where used to train model - it will help when writing report.\n",
            "\n",
            "> **Note:** It lasts for a while - for Częstochowa it is up to 20 minutes\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "09380f82",
         "metadata": {},
         "outputs": [
            {
               "ename": "ModuleNotFoundError",
               "evalue": "No module named 'geopy'",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscraper\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/szkoła/studia/semestr 5/studio projektowe 1/CRoadA/scraper/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/szkoła/studia/semestr 5/studio projektowe 1/CRoadA/scraper/data_loader.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Polygon\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeocoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Nominatim\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munidecode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unidecode \n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable\n",
                  "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'geopy'"
               ]
            }
         ],
         "source": [
            "import random\n",
            "from scraper.data_loader import DataLoader\n",
            "from trainer.model import Model\n",
            "from trainer.trainer import Trainer\n",
            "import os\n",
            "from pathlib import Path\n",
            "\n",
            "random_cities_count = 1\n",
            "\n",
            "folder_path = Path(data_dir)\n",
            "folder_path.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "\n",
            "# List of Polish cities with population between 50,000 and 500,000\n",
            "cities = [\n",
            "    # (\"Warsaw, Polska\", 1790658),\n",
            "    # (\"Kraków, Polska\", 780981),\n",
            "    # (\"Łódź, Polska\", 687702),\n",
            "    # (\"Wrocław, Polska\", 640648),\n",
            "    # (\"Poznań, Polska\", 538633),\n",
            "    # (\"Gdańsk, Polska\", 466630),\n",
            "    (\"Szczecin, Polska\", 402465),\n",
            "    # (\"Bydgoszcz, Polska\", 358928),\n",
            "    (\"Lublin, Polska\", 339433),\n",
            "    (\"Białystok, Polska\", 297459),\n",
            "    (\"Katowice, Polska\", 294510),\n",
            "    # (\"Gdynia, Polska\", 246635),\n",
            "    (\"Częstochowa, Polska\", 224000),\n",
            "    (\"Radom, Polska\", 214000),\n",
            "    (\"Toruń, Polska\", 202074),\n",
            "    (\"Kielce, Polska\", 200000),\n",
            "    (\"Rzeszów, Polska\", 196000),\n",
            "    (\"Opole, Polska\", 128000),\n",
            "    (\"Gliwice, Polska\", 180000),\n",
            "    (\"Zabrze, Polska\", 170000),\n",
            "    (\"Elbląg, Polska\", 120000),\n",
            "    (\"Płock, Polska\", 120000),\n",
            "    (\"Nowy Sącz, Polska\", 85_000),\n",
            "    (\"Słupsk, Polska\", 91_000),\n",
            "    # (\"Świętochłowice, Polska\", 55_000),\n",
            "    (\"Jelenia Góra, Polska\", 79_000),\n",
            "    (\"Stalowa Wola, Polska\", 75_000),\n",
            "    # (\"Koszalin, Polska\", 108_000),\n",
            "    (\"Mielec, Polska\", 59_000),\n",
            "    (\"Legnica, Polska\", 100_000),\n",
            "    (\"Tychy, Polska\", 130_000),\n",
            "    (\"Chorzów, Polska\", 110_000),\n",
            "    # (\"Rybnik, Polska\", 140_000)\n",
            "]\n",
            "\n",
            "# Randomly select 8 cities\n",
            "random_cities = random.sample(cities, random_cities_count)\n",
            "\n",
            "loader = DataLoader(1, 5000, 5000, data_dir=data_dir)\n",
            "managers = []\n",
            "files = []\n",
            "\n",
            "for city, population in random_cities:\n",
            "    file_name = city.replace(\", \", \"-\") + \".city_grid\"\n",
            "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
            "    manager = loader.load_city_grid(city,file_name)\n",
            "    files.append(file_name)\n",
            "    managers.append(manager)\n",
            "    loader.add_elevation_to_grid(manager)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "942e32d6",
         "metadata": {},
         "source": [
            "##### **GRZEGORZ** 1. Model - shallowed unet 256x256, grid density 1 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4794dadf",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "from grid_manager import GridManager\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "64d34e2c",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=50,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f1d77f05",
         "metadata": {},
         "source": [
            "##### 2. Model unet 256x256, grid density 1 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0ca87032",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_256_1m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "bec9d7b1",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f2c52511",
         "metadata": {},
         "source": [
            "##### **Jakub** 3. Model unet 512x512, grid density 1 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0430e553",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=512,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=128,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_512_1m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "92816269",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "dd455342",
         "metadata": {},
         "source": [
            "#### Grid density 2 m\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0a8e0c91",
         "metadata": {},
         "source": [
            "If you have limited disk space, you can adjust `random_cities_count` variable, to download less city grids. It will save time and disk space, but will probably affect model training.\n",
            "If you do this, please write somewhere down, which cities where used to train model - it will help when writing report.\n",
            "\n",
            "> **Note:** It lasts for a while - for Częstochowa it is up to 20 minutes\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "97215bc7",
         "metadata": {},
         "outputs": [],
         "source": [
            "import random\n",
            "from scraper.data_loader import DataLoader\n",
            "from trainer.model import Model\n",
            "from trainer.trainer import Trainer\n",
            "import os\n",
            "from pathlib import Path\n",
            "\n",
            "random_cities_count = 8\n",
            "\n",
            "folder_path = Path(data_dir)\n",
            "folder_path.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "\n",
            "# List of Polish cities with population between 50,000 and 500,000\n",
            "cities = [\n",
            "    # (\"Warsaw, Polska\", 1790658),\n",
            "    # (\"Kraków, Polska\", 780981),\n",
            "    # (\"Łódź, Polska\", 687702),\n",
            "    # (\"Wrocław, Polska\", 640648),\n",
            "    # (\"Poznań, Polska\", 538633),\n",
            "    (\"Gdańsk, Polska\", 466630),\n",
            "    (\"Szczecin, Polska\", 402465),\n",
            "    (\"Bydgoszcz, Polska\", 358928),\n",
            "    (\"Lublin, Polska\", 339433),\n",
            "    (\"Białystok, Polska\", 297459),\n",
            "    (\"Katowice, Polska\", 294510),\n",
            "    (\"Gdynia, Polska\", 246635),\n",
            "    (\"Częstochowa, Polska\", 224000),\n",
            "    (\"Radom, Polska\", 214000),\n",
            "    (\"Toruń, Polska\", 202074),\n",
            "    (\"Kielce, Polska\", 200000),\n",
            "    (\"Rzeszów, Polska\", 196000),\n",
            "    (\"Opole, Polska\", 128000),\n",
            "    (\"Gliwice, Polska\", 180000),\n",
            "    (\"Zabrze, Polska\", 170000),\n",
            "    (\"Elbląg, Polska\", 120000),\n",
            "    (\"Płock, Polska\", 120000),\n",
            "    (\"Nowy Sącz, Polska\", 85_000),\n",
            "    (\"Słupsk, Polska\", 91_000),\n",
            "    (\"Świętochłowice, Polska\", 55_000),\n",
            "    (\"Jelenia Góra, Polska\", 79_000),\n",
            "    (\"Stalowa Wola, Polska\", 75_000),\n",
            "    (\"Koszalin, Polska\", 108_000),\n",
            "    (\"Mielec, Polska\", 59_000),\n",
            "    (\"Legnica, Polska\", 100_000),\n",
            "    (\"Tychy, Polska\", 130_000),\n",
            "    (\"Chorzów, Polska\", 110_000),\n",
            "    (\"Rybnik, Polska\", 140_000)\n",
            "]\n",
            "\n",
            "# Randomly select 8 cities\n",
            "random_cities = random.sample(cities, random_cities_count)\n",
            "\n",
            "loader = DataLoader(2, 2000, 2000, data_dir=data_dir)\n",
            "managers = []\n",
            "files = []\n",
            "\n",
            "for city, population in random_cities:\n",
            "    file_name = city + \".city_grid\"\n",
            "    print(f\"Loading grid of city: {city} to {os.path.join(data_dir, file_name)}\")\n",
            "    manager = loader.load_city_grid(city,file_name)\n",
            "    files.append(file_name)\n",
            "    managers.append(manager)\n",
            "    loader.add_elevation_to_grid(manager)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "805aec81",
         "metadata": {},
         "source": [
            "##### 4. Model unet 256x256, grid density 2 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8d36ac63",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_256_2m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a00ee162",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "30fa874b",
         "metadata": {},
         "source": [
            "##### 5. Model unet 512x512, grid density 2 m\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a8d3ab38",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=512,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=128,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"unet_512_2m\")     # where to save the model\n",
            ")\n",
            "\n",
            "folder = Path(data_dir)\n",
            "files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "print(f\"files: {files}\")\n",
            "\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=files\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c8ef17b7",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=1,             # number of steps per epoch\n",
            "    batch_size=8                   # size of each training batch\n",
            ")\n",
            "# after measuring step duration, adjust steps_per_epoch (max. around 200) and epochs to the amout of time you have"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6f1b8a2d",
         "metadata": {},
         "source": [
            "### Evaluation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "fda74ab0",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2026-02-05 03:11:19.608717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                  "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                  "/media/czlek/228c7df9-4521-43c4-be89-a3306a631e45/asus tuf gaming (mój)/Dokumenty/szkoła/studia/semestr 5/studio projektowe 1/CRoadA/env/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
                  "  if not hasattr(np, \"object\"):\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Starting from file: models/shallowed_unet_256_1m/1768874128_model.keras\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                  "I0000 00:00:1770257485.367305   28650 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4620 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "from pathlib import Path\n",
            "from grid_manager import GridManager\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.SHALLOWED_UNET,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=256,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=os.path.join(\"models\", \"shallowed_unet_256_1m\"),     # where to save the model\n",
            "    input_third_dimension=3,\n",
            "    output_third_dimension=2\n",
            ")\n",
            "\n",
            "# folder = Path(data_dir)\n",
            "# files = [GridManager(f.name, data_dir=data_dir) for f in folder.iterdir() if f.is_file()]\n",
            "# print(f\"files: {files}\")\n",
            "\n",
            "# trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "#     model=model,                # the model defined above\n",
            "#     files=files\n",
            "# )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "2d6690ab",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "displayed: (3000, 3000, 3)\n",
                  "row: 0................row: 1................row: 2................row: 3................row: 4................row: 5................row: 6................row: 7................row: 8................row: 9................row: 10................row: 11................row: 12................row: 13................row: 14................row: 15................"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<matplotlib.image.AxesImage at 0x7fc8690a9940>"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 640x480 with 1 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "from grid_manager import GridManager\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np\n",
            "import time\n",
            "from trainer.model import PREDICT_GRID_INDICES\n",
            "\n",
            "\n",
            "grid_manager = GridManager(\"Bydgoszcz-Polska.city_grid\", data_dir=\"grids/with-is-residential\")\n",
            "metadata = grid_manager.get_metadata()\n",
            "\n",
            "assert metadata.third_dimension_size == 3, \"You have an outdated version of GridManager. Consider downloading grid files again to get all data...\"\n",
            "\n",
            "fragment_row, fragment_col = 500, 500\n",
            "fragment_height, fragment_width = 3000, 3000\n",
            "\n",
            "segment_h, segment_w = metadata.segment_h, metadata.segment_w\n",
            "being_predicted = GridManager(\n",
            "    f\"test_{time.time()}.city_grid\",\n",
            "    fragment_height, fragment_width,\n",
            "    0,0,\n",
            "    metadata.grid_density,\n",
            "    segment_h, segment_w,\n",
            "    data_dir=\"grids/evaluation\",\n",
            "    third_dimension_size=metadata.third_dimension_size\n",
            ")\n",
            "# read_arbitrary_fragment does not take care of memory size - if there are some problems - just use smaller fragment\n",
            "displayed = grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width)\n",
            "print(f\"displayed: {displayed.shape}\")\n",
            "\n",
            "being_predicted.write_arbitrary_fragment(\n",
            "    grid_manager.read_arbitrary_fragment(fragment_row, fragment_col, fragment_height, fragment_width),\n",
            "    0, 0\n",
            ") # for instance some segment in the middle\n",
            "\n",
            "result = model.predict(being_predicted)\n",
            "img = result.read_arbitrary_fragment(\n",
            "    0, 0,\n",
            "    fragment_height - model.get_input_grid_surplus(),\n",
            "    fragment_width - model.get_input_grid_surplus()\n",
            ")[:, :, PREDICT_GRID_INDICES.IS_STREET]\n",
            "\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
            "plt.gray()\n",
            "plt.imshow(dilated)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "953f8fae",
         "metadata": {},
         "source": [
            "## Examples of modules usages\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3402120c",
         "metadata": {},
         "source": [
            "### Download mesh and get grid from OSM\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b0390497",
         "metadata": {},
         "outputs": [],
         "source": [
            "from scraper.data_loader import DataLoader\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2ec41555",
         "metadata": {},
         "outputs": [],
         "source": [
            "# IMPORTANT!! - dataloader checks, if the file already exists\n",
            "from scraper.data_loader import DataLoader\n",
            "\n",
            "cityname = \"Kraków\"\n",
            "\n",
            "loader = DataLoader(1)\n",
            "\n",
            "grid_manager = loader.load_city_grid(cityname, cityname + \".dat\")\n",
            "# loader.add_elevation_to_grid(grid_manager)\n",
            "loader.add_residential_to_grid(grid_manager)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a399ff60",
         "metadata": {},
         "source": [
            "#### Display downloaded grid (dilated to be better visible)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "adba9939",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "import matplotlib.pyplot as plt\n",
            "import cv2\n",
            "import numpy as np\n",
            "\n",
            "grid_manager = GridManager(\"Tychy.dat\", data_dir=\"grids\")\n",
            "img = grid_manager.read_segment(2,0)[:5000, :5000, 0]\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "dilated = cv2.dilate(img, struct_el, iterations=3)\n",
            "plt.gray()\n",
            "plt.imshow(dilated)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "853958a1",
         "metadata": {},
         "source": [
            "#### Adding is residential tests\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "dbaa532c",
         "metadata": {},
         "outputs": [],
         "source": [
            "# IMPORTANT!! - dataloader checks, if the file already exists\n",
            "from scraper.data_loader import DataLoader\n",
            "\n",
            "cityname = \"Tychy\"\n",
            "\n",
            "loader = DataLoader(1)\n",
            "\n",
            "grid_manager = loader.load_city_grid(cityname, cityname + \".dat\")\n",
            "# loader.add_elevation_to_grid(grid_manager)\n",
            "loader.add_residential_to_grid(grid_manager)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a2fa9803",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "from scraper.rasterizer import Rasterizer \n",
            "import matplotlib.pyplot as plt\n",
            "import math\n",
            "\n",
            "grid_manager = GridManager(\"Toruń.dat\")\n",
            "meta = grid_manager.get_metadata()\n",
            "segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
            "segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
            "\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "fig, axes = plt.subplots(segments_rows, segments_cols)\n",
            "fig.set_size_inches((16, 16))\n",
            "axes = axes.flatten()\n",
            "axes_index = 0\n",
            "\n",
            "for row_idx in range(segments_rows):\n",
            "    for col_idx in range(segments_cols):\n",
            "        segment = grid_manager.read_segment(row_idx, col_idx)\n",
            "\n",
            "        axes[axes_index].imshow(segment[:, :, 2], cmap=\"gray\")\n",
            "        axes[axes_index].set_title(f\"Segment_rows: {row_idx}, segment_cols: {col_idx}\")\n",
            "        axes_index += 1"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "69284905",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "from scraper.rasterizer import Rasterizer \n",
            "import matplotlib.pyplot as plt\n",
            "import math\n",
            "\n",
            "grid_manager = GridManager(\"Toruń.dat\")\n",
            "meta = grid_manager.get_metadata()\n",
            "segments_rows = math.ceil(meta.rows_number / meta.segment_h)\n",
            "segments_cols = math.ceil(meta.columns_number / meta.segment_w)\n",
            "\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "fig, axes = plt.subplots(segments_rows, segments_cols)\n",
            "fig.set_size_inches((16, 16))\n",
            "axes = axes.flatten()\n",
            "axes_index = 0\n",
            "\n",
            "for row_idx in range(segments_rows):\n",
            "    for col_idx in range(segments_cols):\n",
            "        segment = grid_manager.read_segment(row_idx, col_idx)\n",
            "\n",
            "        axes[axes_index].imshow(segment[:, :, 0], cmap=\"gray\")\n",
            "        axes[axes_index].set_title(f\"Segment_rows: {row_idx}, segment_cols: {col_idx}\")\n",
            "        axes_index += 1\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c3df3944",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "\n",
            "grid_manager = GridManager(\"Tychy.dat\")\n",
            "img = grid_manager.read_segment(1,1)[:2000, :2000, 0]\n",
            "\n",
            "fig, axs = plt.subplots(2, 2, figsize=(10,5))\n",
            "fig.suptitle(\"Obrazki dróg (zdylatowane, żeby było lepiej widać)\")\n",
            "\n",
            "imgs = [\n",
            "    [grid_manager.read_segment(0,0)[3000:, 3000:, 0], grid_manager.read_segment(0,1)[3000:, :2000, 0]],\n",
            "    [grid_manager.read_segment(1,0)[:2000, 3000:, 0], grid_manager.read_segment(1,1)[:2000, :2000, 0]]\n",
            "]\n",
            "\n",
            "dilated_imgs = []\n",
            "\n",
            "struct_el = np.ones((3,3))\n",
            "\n",
            "for imgs_row in imgs:\n",
            "    target_row = []\n",
            "    dilated_imgs.append(target_row)\n",
            "    for img in imgs_row:\n",
            "        target_row.append(cv2.dilate(img, struct_el, iterations=3))\n",
            "\n",
            "plt.gray()\n",
            "axs[0,0].imshow(dilated_imgs[0][0])\n",
            "axs[0,1].imshow(dilated_imgs[0][1])\n",
            "axs[1,0].imshow(dilated_imgs[1][0])\n",
            "axs[1,1].imshow(dilated_imgs[1][1])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "863446e5",
         "metadata": {},
         "outputs": [],
         "source": [
            "from scraper.grid_builder import GridBuilder\n",
            "import math\n",
            "from scraper.rasterizer import Rasterizer\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "city = \"Tychy, Polska\"\n",
            "builder = GridBuilder()\n",
            "\n",
            "gdf_edges = builder.get_city_roads(city)\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "\n",
            "grid_2d = rasterizer.get_rasterize_roads(gdf_edges, 1, is_residential=False)\n",
            "plt.imshow(grid_2d, cmap=\"gray\")\n",
            "plt.title(f\"City: {city}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "af730340",
         "metadata": {},
         "outputs": [],
         "source": [
            "from scraper.grid_builder import GridBuilder\n",
            "import math\n",
            "from scraper.rasterizer import Rasterizer\n",
            "import matplotlib.pyplot as plt\n",
            "import geopandas as gpd\n",
            "\n",
            "city = \"Kraków\"\n",
            "builder = GridBuilder()\n",
            "grid_density = 1\n",
            "segment_h = 5000\n",
            "segment_w = 5000\n",
            "data_dir = \"grids\"\n",
            "\n",
            "gdf_edges = builder.get_city_roads(city)\n",
            "min_x, min_y, max_x, max_y = gdf_edges.total_bounds\n",
            "\n",
            "columns_number = math.ceil((max_x - min_x) / grid_density)\n",
            "rows_number = math.ceil((max_y - min_y) / grid_density)\n",
            "\n",
            "segment_rows = math.ceil((rows_number) / segment_h)\n",
            "segment_cols = math.ceil((columns_number) / segment_h)\n",
            "\n",
            "# grid_manager = GridManager(file_name, rows_number=int(rows_number), columns_number=int(columns_number),\n",
            "#                             grid_density=grid_density, segment_h=segment_h, segment_w=segment_w,\n",
            "#                             data_dir=data_dir, upper_left_longitude=min_x, upper_left_latitude=max_y)\n",
            "print(f\"Height: {int(rows_number)}, Width: {int(columns_number)}, rows: {segment_rows}, cols: {segment_cols}\")\n",
            "\n",
            "rasterizer = Rasterizer()\n",
            "fig, axes = plt.subplots(segment_rows, segment_cols)\n",
            "fig.set_size_inches((16, 16))\n",
            "axes = axes.flatten()\n",
            "axes_index = 0\n",
            "# single_row_gdf = gdf_edges.sample(1)\n",
            "\n",
            "# grid_2d = rasterizer.rasterize_segment_from_indexes(gdf_edges=single_row_gdf, indexes=(0, 0), is_residential=False\n",
            "#                                                             size_h=segment_h, size_w=segment_w,\n",
            "#                                                             pixel_size=grid_density)\n",
            "# print(grid_2d.shape)\n",
            "# plt.imshow(grid_2d, cmap=\"gray\")\n",
            "\n",
            "for i in range(segment_rows):\n",
            "    for j in range(segment_cols):\n",
            "        grid_2d = rasterizer.rasterize_segment_from_indexes(gdf_edges=gdf_edges, indexes=(i, j), is_residential=False,\n",
            "                                                            size_h=segment_h, size_w=segment_w,\n",
            "                                                            pixel_size=grid_density)\n",
            "        axes[axes_index].imshow(grid_2d, cmap=\"gray\")\n",
            "        axes[axes_index].set_title(f\"Segment_rows: {i}, segment_cols: {j}\")\n",
            "        axes_index += 1"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a3f8acb1",
         "metadata": {},
         "source": [
            "### Data management\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1de7261b",
         "metadata": {},
         "outputs": [],
         "source": [
            "from grid_manager import GridManager\n",
            "import numpy as np\n",
            "\n",
            "filename = \"przyklad1.dat\"\n",
            "\n",
            "src_man = GridManager(filename, 2000, 2000, 0.0, 0.0, 1, 3, 3, data_dir=\"grids\")\n",
            "\n",
            "# a = np.zeros((2000, 2000, 2), dtype=np.float64)\n",
            "is_street = np.array([\n",
            "    [1, 2, 3, 4, 5, 6],\n",
            "    [11, 12, 13, 14, 15, 16],\n",
            "    [21, 22, 23, 24, 25, 26],\n",
            "    [31, 32, 33, 34, 35, 36],\n",
            "    [41, 42, 43, 44, 45, 46],\n",
            "    [51, 52, 53, 54, 55, 56],\n",
            "])\n",
            "a = np.zeros((is_street.shape[0], is_street.shape[1], 3))\n",
            "a[:,:, 0] = is_street\n",
            "\n",
            "for x in range(2):\n",
            "    for y in range(2):\n",
            "        src_man.write_segment(a[y * 3: (y + 1) * 3, x*3:(x + 1) * 3], y, x)\n",
            "\n",
            "\n",
            "src_man.write_segment(a[:2,:3, :] + 1, 666, 0)\n",
            "src_man.write_segment(a[:3,:2, :] + 2, 0, 666)\n",
            "\n",
            "man = src_man.deep_copy()\n",
            "\n",
            "print(f\"1: {man.read_segment(0, 0)[:, :, 0]}\")\n",
            "print(f\"2: {man.read_segment(0, 1)[:, :, 0]}\")\n",
            "print(f\"3: {man.read_segment(1, 0)[:, :, 0]}\")\n",
            "print(f\"4: {man.read_segment(1, 1)[:, :, 0]}\")\n",
            "print(f\"5: {man.read_segment(666, 666)[:, :, 0]}\")\n",
            "\n",
            "print(man.get_metadata())\n",
            "\n",
            "man.delete()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c1a46640",
         "metadata": {},
         "source": [
            "### Streets discovery\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0770ae13",
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def show_crossroads(img_height, img_width, conflictless_crossroads, conflicting_crossroads):\n",
            "    fig, axs = plt.subplots(  max(2, len(conflictless_crossroads), len(conflicting_crossroads)), 6, figsize=(10,5))\n",
            "    axs[0, 0].set_title(\"conflictless\")\n",
            "    axs[0, 1].set_title(\"junctions\")\n",
            "    axs[0, 2].set_title(\"conflicts\")\n",
            "    axs[0, 3].set_title(\"conflicting\")\n",
            "    axs[0, 4].set_title(\"junctions\")\n",
            "    axs[0, 5].set_title(\"conflicts\")\n",
            "\n",
            "\n",
            "    for i in range(len(conflictless_crossroads)):\n",
            "        crossroad = conflictless_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 0].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflictless_crossroads)):\n",
            "        crossroad = conflictless_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for street in crossroad.street_junctions.keys():\n",
            "            crossroad_image[crossroad.street_junctions[street]] = 1\n",
            "\n",
            "        axs[i, 1].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflictless_crossroads)):\n",
            "        crossroad = conflictless_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.conflicting_points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 2].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflicting_crossroads)):\n",
            "        crossroad = conflicting_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 3].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflicting_crossroads)):\n",
            "        crossroad = conflicting_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for street in crossroad.street_junctions.keys():\n",
            "            crossroad_image[crossroad.street_junctions[street]] = 1\n",
            "\n",
            "        axs[i, 4].imshow(crossroad_image)\n",
            "\n",
            "    for i in range(len(conflicting_crossroads)):\n",
            "        crossroad = conflicting_crossroads[i]\n",
            "\n",
            "        crossroad_image = np.zeros((img_height, img_width))\n",
            "        for point in crossroad.conflicting_points:\n",
            "            crossroad_image[point] = 1\n",
            "\n",
            "        axs[i, 5].imshow(crossroad_image)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "131fd71e",
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def show_streets(img_height, img_width, conflictless_streets, conflicting_streets):\n",
            "    fig, axs = plt.subplots(  max(2, len(conflictless_streets), len(conflicting_streets)), 4, figsize=(10,8))\n",
            "    axs[0, 0].set_title(\"conflictless\")\n",
            "    axs[0, 1].set_title(\"conflicts\")\n",
            "    axs[0, 2].set_title(\"conflicting\")\n",
            "    axs[0, 3].set_title(\"conflicts\")\n",
            "\n",
            "\n",
            "    for i in range(len(conflictless_streets)):\n",
            "        street = conflictless_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.linestring:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 0].imshow(street_image)\n",
            "    \n",
            "    for i in range(len(conflictless_streets)):\n",
            "        street = conflictless_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.conflicts:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 1].imshow(street_image)\n",
            "\n",
            "    for i in range(len(conflicting_streets)):\n",
            "        street = conflicting_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.linestring:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 2].imshow(street_image)\n",
            "    \n",
            "    for i in range(len(conflicting_streets)):\n",
            "        street = conflicting_streets[i]\n",
            "\n",
            "        street_image = np.zeros((img_height, img_width))\n",
            "        for point in street.conflicts:\n",
            "            street_image[point] = 1\n",
            "\n",
            "        axs[i, 3].imshow(street_image)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "25f547aa",
         "metadata": {},
         "source": [
            "#### Diamond\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7fc0609f",
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "from skimage.morphology import skeletonize\n",
            "from graph_remaker.morphological_remaker import discover_streets\n",
            "import cv2\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "image = cv2.imread(\"test_images/diamond.png\", cv2.IMREAD_GRAYSCALE)\n",
            "plt.gray()\n",
            "bin_image = image > 0\n",
            "# plt.imshow(bin_image * 255)\n",
            "\n",
            "processed_image = bin_image[:, :, np.newaxis]\n",
            "processed_image = np.concatenate((processed_image, np.zeros(processed_image.shape)), axis=2)\n",
            "\n",
            "conflictless_crossroads, conflicting_crossroads, conflictless_streets, conflicting_streets = discover_streets(processed_image)\n",
            "\n",
            "height, width = image.shape\n",
            "plt.imshow(image)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e87e4f55",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_crossroads(height, width, conflictless_crossroads, conflicting_crossroads)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a7a21635",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_streets(height, width, conflictless_streets, conflicting_streets)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "7b0960ab",
         "metadata": {},
         "source": [
            "#### Conflicting crossroad and dead-end\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9d3b7375",
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "from skimage.morphology import skeletonize\n",
            "from graph_remaker.morphological_remaker import discover_streets\n",
            "import cv2\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "image = cv2.imread(\"test_images/crossroad_conflict_deadend.png\", cv2.IMREAD_GRAYSCALE)\n",
            "plt.gray()\n",
            "bin_image = image > 0\n",
            "# plt.imshow(bin_image * 255)\n",
            "\n",
            "processed_image = bin_image[:, :, np.newaxis]\n",
            "processed_image = np.concatenate((processed_image, np.zeros(processed_image.shape)), axis=2)\n",
            "\n",
            "conflictless_crossroads, conflicting_crossroads, conflictless_streets, conflicting_streets = discover_streets(processed_image)\n",
            "\n",
            "height, width = image.shape\n",
            "plt.imshow(image)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3b7ed360",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_crossroads(width, height, conflictless_crossroads, conflicting_crossroads)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c63a915d",
         "metadata": {},
         "outputs": [],
         "source": [
            "show_streets(width, height, conflictless_streets, conflicting_streets)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8e175b48",
         "metadata": {},
         "source": [
            "### Model training\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a11c66cd",
         "metadata": {},
         "source": [
            "##### Use the whole VRAM\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b2ee2324",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import tf\n",
            "\n",
            "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
            "if gpus:\n",
            "    try:\n",
            "        for gpu in gpus:\n",
            "            tf.config.experimental.set_memory_growth(gpu, False)\n",
            "        tf.config.set_logical_device_configuration(\n",
            "            gpus[0],\n",
            "            [tf.config.LogicalDeviceConfiguration(memory_limit=4000)] # Limit to 4000 MB of VRAM - TODO: adjust based on your GPU\n",
            "        )\n",
            "    except RuntimeError as e:\n",
            "        print(e)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e6e2a0e3",
         "metadata": {},
         "source": [
            "##### Create batch sequence of clippings that are the candidates for a model and print the shape of the input\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d51faed8",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_sequence import ClippingBatchSequence # not used anymore in the project - only for visualisation\n",
            "from trainer.batch_sequence import BatchSequence            # not used anymore in the project - only for visualisation\n",
            "\n",
            "batchSeq = ClippingBatchSequence(           # Define a clipping batch sequence\n",
            "            BatchSequence(                      # Define a batch sequence\n",
            "                files=list([\"Tychy.dat\"]),          # list of files to use\n",
            "                batch_size=1,                       # not important here\n",
            "                cut_sizes=[(512, 512)],             # sizes of cuttings to make from the grid\n",
            "            ),\n",
            "            clipping_size=512,                  # size of the clipping (input to the model)\n",
            "            input_grid_surplus=64,              # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "        )\n",
            "\n",
            "X, Y = batchSeq[0]          # get the first batch\n",
            "print(\"Shape of the input batch: \", X.shape)              # print the shape of the input batch"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2ce9534c",
         "metadata": {},
         "source": [
            "##### Display example clipping (X and y) - `WARNING: you may need to run the above code up to approx. 3-5 times in order to get an image below, other than a void one`\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3e4ae7aa",
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
            "\n",
            "axs[0].imshow(X[0, :, :, 0], cmap='grey')\n",
            "axs[1].imshow(Y[\"is_street\"][0, :, :, 0], cmap='grey')\n",
            "\n",
            "print(\"X min/max:\", X[0, :, :, 0].min(), X[0, :, :, 0].max())\n",
            "print(\"Y min/max:\", Y[\"is_street\"][0, :, :, 0].min(), Y[\"is_street\"][0, :, :, 0].max())"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ba1a9a19",
         "metadata": {},
         "source": [
            "##### Show available GPUs\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "20aabc2d",
         "metadata": {},
         "outputs": [],
         "source": [
            "from tensorflow.python.client import device_lib\n",
            "import tensorflow as tf\n",
            "\n",
            "def get_available_devices():\n",
            "    local_device_protos = device_lib.list_local_devices()\n",
            "    return [x.name for x in local_device_protos]\n",
            "\n",
            "print(get_available_devices())\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "11892b0d",
         "metadata": {},
         "source": [
            "##### Define a model and then cities for the trainer (e.g. Tychy) as the files list\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d17e40c5",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels       # Import ClipModels Enum\n",
            "\n",
            "print(\"Possible models:\")\n",
            "for model in ClipModels:\n",
            "    print(f\"- {model.name}: {model.value}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "6716a8f5",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "\n",
            "model = ClippingModel(      # Define the model\n",
            "    ClipModels.BASE,            # choose a model from ClipModels Enum (run the cell above to see possible models)\n",
            "    clipping_size=512,          # size of the clipping (input to the model)\n",
            "    clipping_surplus=64,        # surplus around the clipping (model gives an output smaller than input, so surplus is needed)\n",
            "    path=\"model_test_16_01\"     # where to save the model\n",
            ")\n",
            "trainer = Trainer(          # Initialize the trainer with the model and data files\n",
            "    model=model,                # the model defined above\n",
            "    files=[                     # list of data files to use for training\n",
            "        \"Tychy.dat\",\n",
            "        # \"Kraków.dat\"\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f8c5b859",
         "metadata": {},
         "source": [
            "##### Run fitting which created the above batch sequence and trains the model\n",
            "\n",
            "`random_fit_from_files(self, epochs: int = 100, steps_per_epoch=1000, batch_size=32)`\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "30bd51e2",
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.random_fit_from_files(  # Train the model with random data from files\n",
            "    epochs=1,                       # number of epochs to train\n",
            "    steps_per_epoch=10,             # number of steps per epoch\n",
            "    batch_size=32                   # size of each training batch\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "151c07e9",
         "metadata": {},
         "source": [
            "##### Other models etc.\n",
            "\n",
            "You may need to run \"Use the whole VRAM\" cell before them (separately and probably after the kernel restart)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b880c096",
         "metadata": {},
         "source": [
            "#### UNET\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b222d447",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
            "\n",
            "model = ClippingModel(\n",
            "    ClipModels.UNET,\n",
            "    clipping_size=512,\n",
            "    clipping_surplus=64,\n",
            "    path=\"model_test_13_01_13_22\"\n",
            ")\n",
            "trainer = Trainer(\n",
            "    model=model,\n",
            "    files=[\n",
            "        \"Tychy.dat\",\n",
            "        \"Kraków.dat\"\n",
            "    ]\n",
            ")\n",
            "# Na mojej laptopowej RTX 3050 Ti 4GB, batch_size=8 nie poradził sobie z pamięcią po 4 minutach.\n",
            "# Zmieniłem batch_size na 2 i działa - 44 sekundy na poniższym przykładzie\n",
            "# UPDATE: Zmieniłem na 4, działa dobrze\n",
            "# - trochę ponad minuta po ustawieniu (w wyższej komórce) memory_limit=4000\n",
            "trainer.random_fit_from_files(1, 10, batch_size=4)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b5f82b17",
         "metadata": {},
         "source": [
            "#### AlexInspired\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "dd0da5d8",
         "metadata": {},
         "outputs": [],
         "source": [
            "from trainer.clipping_model import ClipModels, ClippingModel, tf\n",
            "from trainer.trainer import Trainer\n",
            "\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
            "\n",
            "model = ClippingModel(\n",
            "    ClipModels.ALEX_INSPIRED,\n",
            "    clipping_size=256,\n",
            "    clipping_surplus=32,\n",
            "    path=\"model_test_13_01_13_22\"\n",
            ")\n",
            "trainer = Trainer(\n",
            "    model=model,\n",
            "    files=[\n",
            "        \"Tychy.dat\",\n",
            "        \"Kraków.dat\"\n",
            "    ]\n",
            ")\n",
            "\n",
            "# u mnie zajęło 41 minut i 1.1 sekundy\n",
            "trainer.random_fit_from_files(1, 10, batch_size=4)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a864c4df36b3e3e0",
         "metadata": {},
         "source": [
            "### upper algorithm\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3ceac6721d76834b",
         "metadata": {
            "ExecuteTime": {
               "end_time": "2026-01-13T20:22:07.222397Z",
               "start_time": "2026-01-13T20:21:50.479488Z"
            }
         },
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import networkx as nx\n",
            "import os\n",
            "import cv2\n",
            "import numpy as np\n",
            "import math\n",
            "from grid_manager import GridManager, GRID_INDICES\n",
            "from graph_remaker.memory_wise import process_large_grid\n",
            "\n",
            "# 1. GENEROWANIE SZTUCZNEGO OBRAZU\n",
            "H, W = 1000, 1000\n",
            "SEG_SIZE = 200\n",
            "TEMP_GRID_FILE = \"square_test.dat\"\n",
            "\n",
            "\n",
            "print(f\"Generowanie obrazu {W}x{H}...\")\n",
            "img = np.zeros((H, W), dtype=np.uint8)\n",
            "\n",
            "# Rysujemy KWADRAT (linie prostopadłe do osi)\n",
            "# Współrzędne: (150, 150) do (850, 850)\n",
            "# Linie będą przecinać granice segmentów (które są co 200px) idealnie prosto.\n",
            "cv2.rectangle(img, (150, 150), (850, 850), 255, thickness=10)\n",
            "\n",
            "# Dodajmy Krzyż w środku (żeby przetestować skrzyżowania typu \"4-way\")\n",
            "cv2.line(img, (500, 150), (500, 850), 255, thickness=10) # Pion\n",
            "cv2.line(img, (150, 500), (850, 500), 255, thickness=10) # Poziom\n",
            "\n",
            "print(\"Obraz wygenerowany.\")\n",
            "\n",
            "# 2. ZAPIS DO PLIKU\n",
            "if os.path.exists(os.path.join(\"grids\", TEMP_GRID_FILE)):\n",
            "    os.remove(os.path.join(\"grids\", TEMP_GRID_FILE))\n",
            "\n",
            "gm = GridManager(TEMP_GRID_FILE, H, W,\n",
            "                 upper_left_longitude=0.0, upper_left_latitude=0.0,\n",
            "                 grid_density=1.0,\n",
            "                 segment_h=SEG_SIZE, segment_w=SEG_SIZE)\n",
            "\n",
            "img_binary = (img > 0).astype(np.float32)\n",
            "\n",
            "rows_n = math.ceil(H / SEG_SIZE)\n",
            "cols_n = math.ceil(W / SEG_SIZE)\n",
            "\n",
            "print(f\"Dzielenie na {rows_n}x{cols_n} segmentów...\")\n",
            "\n",
            "for r in range(rows_n):\n",
            "    for c in range(cols_n):\n",
            "        y_start = r * SEG_SIZE\n",
            "        x_start = c * SEG_SIZE\n",
            "        y_end = min(y_start + SEG_SIZE, H)\n",
            "        x_end = min(x_start + SEG_SIZE, W)\n",
            "\n",
            "        chunk = img_binary[y_start:y_end, x_start:x_end]\n",
            "        gm.write_segment(chunk, r, c)\n",
            "\n",
            "print(\"Dane zapisane. Uruchamiam memory_wise...\")\n",
            "\n",
            "# 3. URUCHOMIENIE\n",
            "try:\n",
            "    # Używamy tej wersji memory_wise, którą masz teraz (z patchem i snappingiem)\n",
            "    graph = process_large_grid(TEMP_GRID_FILE)\n",
            "\n",
            "    # 4. WIZUALIZACJA\n",
            "    print(f\"\\n--- WYNIK ---\")\n",
            "    print(f\"Węzły: {len(graph.nodes)}\")\n",
            "    print(f\"Krawędzie: {len(graph.edges)}\")\n",
            "\n",
            "    plt.figure(figsize=(12, 6))\n",
            "\n",
            "    # Lewa: Obraz\n",
            "    plt.subplot(1, 2, 1)\n",
            "    plt.title(f\"Idealny Kwadrat (Linie proste)\")\n",
            "    plt.imshow(img, cmap='gray')\n",
            "\n",
            "    # Siatka cięcia\n",
            "    for y in range(0, H, SEG_SIZE):\n",
            "        plt.axhline(y, color='yellow', linestyle='--', alpha=0.3)\n",
            "    for x in range(0, W, SEG_SIZE):\n",
            "        plt.axvline(x, color='yellow', linestyle='--', alpha=0.3)\n",
            "\n",
            "    # Prawa: Graf\n",
            "    plt.subplot(1, 2, 2)\n",
            "    plt.title(\"Zrekonstruowany Graf\")\n",
            "\n",
            "    if len(graph.nodes) > 0:\n",
            "        pos = {n: (data['x'], data['y']) for n, data in graph.nodes(data=True)}\n",
            "\n",
            "        nx.draw(graph, pos,\n",
            "                node_size=20,\n",
            "                node_color='red',\n",
            "                edge_color='cyan',\n",
            "                width=2,\n",
            "                with_labels=False,\n",
            "                arrows=False)\n",
            "        plt.axis('equal')\n",
            "        plt.gca().invert_yaxis() # Żeby góra była na górze\n",
            "    else:\n",
            "        plt.text(0.5, 0.5, \"BRAK WYNIKÓW\", ha='center', color='red', fontsize=15)\n",
            "\n",
            "    plt.tight_layout()\n",
            "    plt.show()\n",
            "\n",
            "except Exception as e:\n",
            "    print(f\"CRITICAL ERROR: {e}\")\n",
            "    import traceback\n",
            "    traceback.print_exc()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "env",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
